import gradio as gr
from pdfminer.high_level import extract_text_to_fp
from sentence_transformers import SentenceTransformer
# å¯¼å…¥äº¤å‰ç¼–ç å™¨
from sentence_transformers import CrossEncoder
import chromadb
from chromadb.config import Settings
import requests
import json
from io import StringIO
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import socket
import webbrowser
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time
from datetime import datetime
import hashlib
import re
from dotenv import load_dotenv
# å¯¼å…¥BM25ç®—æ³•åº“
from rank_bm25 import BM25Okapi
import numpy as np
import jieba
import threading
from functools import lru_cache

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
SERPAPI_KEY = os.getenv("SERPAPI_KEY")  # åœ¨.envä¸­è®¾ç½® SERPAPI_KEY
SEARCH_ENGINE = "google"  # å¯æ ¹æ®éœ€è¦æ”¹ä¸ºå…¶ä»–æœç´¢å¼•æ“
# æ–°å¢ï¼šé‡æ’åºæ–¹æ³•é…ç½®ï¼ˆäº¤å‰ç¼–ç å™¨æˆ–LLMï¼‰
RERANK_METHOD = os.getenv("RERANK_METHOD", "cross_encoder")  # "cross_encoder" æˆ– "llm"
# æ–°å¢ï¼šSiliconFlow APIé…ç½®
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY", "sk-lnflogwkgcgchrztauesjderjgjqmwldwtxwkkfwzcnshbgf")
SILICONFLOW_API_URL = os.getenv("SILICONFLOW_API_URL", "https://api.siliconflow.cn/v1/chat/completions")

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ è¶…æ—¶è®¾ç½®
import requests
requests.adapters.DEFAULT_RETRIES = 3  # å¢åŠ é‡è¯•æ¬¡æ•°

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ç¯å¢ƒå˜é‡è®¾ç½®
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # ç¦ç”¨oneDNNä¼˜åŒ–

# åœ¨æ–‡ä»¶æœ€å¼€å¤´æ·»åŠ ä»£ç†é…ç½®
import os
os.environ['NO_PROXY'] = 'localhost,127.0.0.1'  # æ–°å¢ä»£ç†ç»•è¿‡è®¾ç½®

# åˆå§‹åŒ–ç»„ä»¶
EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
CHROMA_CLIENT = chromadb.PersistentClient(
    path="./chroma_db",
    settings=chromadb.Settings(anonymized_telemetry=False)
)
COLLECTION = CHROMA_CLIENT.get_or_create_collection("rag_docs")

# æ–°å¢ï¼šäº¤å‰ç¼–ç å™¨åˆå§‹åŒ–ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
cross_encoder = None
cross_encoder_lock = threading.Lock()

def get_cross_encoder():
    """å»¶è¿ŸåŠ è½½äº¤å‰ç¼–ç å™¨æ¨¡å‹"""
    global cross_encoder
    if cross_encoder is None:
        with cross_encoder_lock:
            if cross_encoder is None:
                try:
                    # ä½¿ç”¨å¤šè¯­è¨€äº¤å‰ç¼–ç å™¨ï¼Œæ›´é€‚åˆä¸­æ–‡
                    cross_encoder = CrossEncoder('sentence-transformers/distiluse-base-multilingual-cased-v2')
                    logging.info("äº¤å‰ç¼–ç å™¨åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logging.error(f"åŠ è½½äº¤å‰ç¼–ç å™¨å¤±è´¥: {str(e)}")
                    # è®¾ç½®ä¸ºNoneï¼Œä¸‹æ¬¡è°ƒç”¨ä¼šé‡è¯•
                    cross_encoder = None
    return cross_encoder

# æ–°å¢ï¼šBM25ç´¢å¼•ç®¡ç†
def recursive_retrieval(initial_query, max_iterations=3, enable_web_search=False, model_choice="ollama"):
    """
    å®ç°é€’å½’æ£€ç´¢ä¸è¿­ä»£æŸ¥è¯¢åŠŸèƒ½
    é€šè¿‡åˆ†æå½“å‰æŸ¥è¯¢ç»“æœï¼Œç¡®å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢
    
    Args:
        initial_query: åˆå§‹æŸ¥è¯¢
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
        model_choice: ä½¿ç”¨çš„æ¨¡å‹é€‰æ‹©("ollama"æˆ–"siliconflow")
        
    Returns:
        åŒ…å«æ‰€æœ‰æ£€ç´¢å†…å®¹çš„åˆ—è¡¨
    """
    query = initial_query
    all_contexts = []
    all_doc_ids = []
    all_metadata = []
    
    for i in range(max_iterations):
        logging.info(f"é€’å½’æ£€ç´¢è¿­ä»£ {i+1}/{max_iterations}ï¼Œå½“å‰æŸ¥è¯¢: {query}")
        
        # å¦‚æœå¯ç”¨äº†è”ç½‘æœç´¢ï¼Œå…ˆè¿›è¡Œç½‘ç»œæœç´¢
        web_results = []
        if enable_web_search and check_serpapi_key():
            try:
                web_results = update_web_results(query)
            except Exception as e:
                logging.error(f"ç½‘ç»œæœç´¢é”™è¯¯: {str(e)}")
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = EMBED_MODEL.encode([query]).tolist()
        
        # è¯­ä¹‰å‘é‡æ£€ç´¢
        try:
            semantic_results = COLLECTION.query(
                query_embeddings=query_embedding,
                n_results=10,
                include=['documents', 'metadatas']
            )
        except Exception as e:
            logging.error(f"å‘é‡æ£€ç´¢é”™è¯¯: {str(e)}")
            semantic_results = {"ids": [[]], "documents": [[]], "metadatas": [[]], "distances": [[]]}
        
        # BM25å…³é”®è¯æ£€ç´¢
        bm25_results = BM25_MANAGER.search(query, top_k=10)
        
        # æ··åˆæ£€ç´¢ç»“æœ
        hybrid_results = hybrid_merge(semantic_results, bm25_results, alpha=0.7)
        
        # æå–ç»“æœ
        doc_ids = []
        docs = []
        metadata_list = []
        
        if hybrid_results:
            for doc_id, result_data in hybrid_results[:10]:
                doc_ids.append(doc_id)
                docs.append(result_data['content'])
                metadata_list.append(result_data['metadata'])
        
        # é‡æ’åºç»“æœ
        if docs:
            try:
                reranked_results = rerank_results(query, docs, doc_ids, metadata_list, top_k=5)
            except Exception as e:
                logging.error(f"é‡æ’åºé”™è¯¯: {str(e)}")
                reranked_results = [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0}) 
                                  for doc_id, doc, meta in zip(doc_ids, docs, metadata_list)]
        else:
            reranked_results = []
        
        # æ”¶é›†å½“å‰è¿­ä»£çš„ç»“æœ
        current_contexts = []
        for doc_id, result_data in reranked_results:
            doc = result_data['content']
            metadata = result_data['metadata']
            
            # æ·»åŠ åˆ°æ€»ç»“æœé›†
            if doc_id not in all_doc_ids:  # é¿å…é‡å¤
                all_doc_ids.append(doc_id)
                all_contexts.append(doc)
                all_metadata.append(metadata)
                current_contexts.append(doc)
        
        # å¦‚æœå·²ç»æ˜¯æœ€åä¸€æ¬¡è¿­ä»£ï¼Œç»“æŸå¾ªç¯
        if i == max_iterations - 1:
            break
            
        # ä½¿ç”¨LLMåˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢
        if current_contexts:
            # ç®€å•æ€»ç»“å½“å‰æ£€ç´¢å†…å®¹
            current_summary = "\n".join(current_contexts[:3]) if current_contexts else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
            
            next_query_prompt = f"""åŸºäºåŸå§‹é—®é¢˜: {initial_query}
ä»¥åŠå·²æ£€ç´¢ä¿¡æ¯: 
{current_summary}

åˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢ã€‚å¦‚æœéœ€è¦ï¼Œè¯·æä¾›æ–°çš„æŸ¥è¯¢é—®é¢˜ï¼Œä½¿ç”¨ä¸åŒè§’åº¦æˆ–æ›´å…·ä½“çš„å…³é”®è¯ã€‚
å¦‚æœå·²ç»æœ‰å……åˆ†ä¿¡æ¯ï¼Œè¯·å›å¤'ä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢'ã€‚

æ–°æŸ¥è¯¢(å¦‚æœéœ€è¦):"""
            
            try:
                # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„API
                if model_choice == "siliconflow":
                    # ä½¿ç”¨SiliconFlow API
                    logging.info("ä½¿ç”¨SiliconFlow APIåˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢")
                    next_query_result = call_siliconflow_api(next_query_prompt, temperature=0.7, max_tokens=256)
                    
                    # å»é™¤å¯èƒ½çš„æ€ç»´é“¾æ ‡è®°
                    if "<think>" in next_query_result:
                        next_query = next_query_result.split("<think>")[0].strip()
                    else:
                        next_query = next_query_result
                else:
                    # ä½¿ç”¨æœ¬åœ°Ollama
                    logging.info("ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹åˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢")
                    response = session.post(
                        "http://localhost:11434/api/generate",
                        json={
                            "model": "deepseek-r1:1.5b",
                            "prompt": next_query_prompt,
                            "stream": False
                        },
                        timeout=30
                    )
                    next_query = response.json().get("response", "")
                
                if "ä¸éœ€è¦" in next_query or "ä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢" in next_query or len(next_query.strip()) < 5:
                    logging.info("LLMåˆ¤æ–­ä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢ï¼Œç»“æŸé€’å½’æ£€ç´¢")
                    break
                    
                # ä½¿ç”¨æ–°æŸ¥è¯¢ç»§ç»­è¿­ä»£
                query = next_query
                logging.info(f"ç”Ÿæˆæ–°æŸ¥è¯¢: {query}")
            except Exception as e:
                logging.error(f"ç”Ÿæˆæ–°æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")
                break
        else:
            # å¦‚æœå½“å‰è¿­ä»£æ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ï¼Œç»“æŸè¿­ä»£
            break
    
    return all_contexts, all_doc_ids, all_metadata

class BM25IndexManager:
    def __init__(self):
        self.bm25_index = None
        self.doc_mapping = {}  # æ˜ å°„BM25ç´¢å¼•ä½ç½®åˆ°æ–‡æ¡£ID
        self.tokenized_corpus = []
        self.raw_corpus = []
        
    def build_index(self, documents, doc_ids):
        """æ„å»ºBM25ç´¢å¼•"""
        self.raw_corpus = documents
        self.doc_mapping = {i: doc_id for i, doc_id in enumerate(doc_ids)}
        
        # å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯ï¼Œä½¿ç”¨jiebaåˆ†è¯å™¨æ›´é€‚åˆä¸­æ–‡
        self.tokenized_corpus = []
        for doc in documents:
            # å¯¹ä¸­æ–‡æ–‡æ¡£è¿›è¡Œåˆ†è¯
            tokens = list(jieba.cut(doc))
            self.tokenized_corpus.append(tokens)
        
        # åˆ›å»ºBM25ç´¢å¼•
        self.bm25_index = BM25Okapi(self.tokenized_corpus)
        return True
        
    def search(self, query, top_k=5):
        """ä½¿ç”¨BM25æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.bm25_index:
            return []
        
        # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        tokenized_query = list(jieba.cut(query))
        
        # è·å–BM25å¾—åˆ†
        bm25_scores = self.bm25_index.get_scores(tokenized_query)
        
        # è·å–å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£ç´¢å¼•
        top_indices = np.argsort(bm25_scores)[-top_k:][::-1]
        
        # è¿”å›ç»“æœ
        results = []
        for idx in top_indices:
            if bm25_scores[idx] > 0:  # åªè¿”å›æœ‰ç›¸å…³æ€§çš„ç»“æœ
                results.append({
                    'id': self.doc_mapping[idx],
                    'score': float(bm25_scores[idx]),
                    'content': self.raw_corpus[idx]
                })
        
        return results
    
    def clear(self):
        """æ¸…ç©ºç´¢å¼•"""
        self.bm25_index = None
        self.doc_mapping = {}
        self.tokenized_corpus = []
        self.raw_corpus = []

# åˆå§‹åŒ–BM25ç´¢å¼•ç®¡ç†å™¨
BM25_MANAGER = BM25IndexManager()

logging.basicConfig(level=logging.INFO)

print("Gradio version:", gr.__version__)  # æ·»åŠ ç‰ˆæœ¬è¾“å‡º

# åœ¨åˆå§‹åŒ–ç»„ä»¶åæ·»åŠ ï¼š
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
session.mount('http://', HTTPAdapter(max_retries=retries))

#########################################
# SerpAPI ç½‘ç»œæŸ¥è¯¢åŠå‘é‡åŒ–å¤„ç†å‡½æ•°
#########################################
def serpapi_search(query: str, num_results: int = 5) -> list:
    """
    æ‰§è¡Œ SerpAPI æœç´¢ï¼Œå¹¶è¿”å›è§£æåçš„ç»“æ„åŒ–ç»“æœ
    """
    if not SERPAPI_KEY:
        raise ValueError("æœªè®¾ç½® SERPAPI_KEY ç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ API å¯†é’¥ã€‚")
    try:
        params = {
            "engine": SEARCH_ENGINE,
            "q": query,
            "api_key": SERPAPI_KEY,
            "num": num_results,
            "hl": "zh-CN",  # ä¸­æ–‡ç•Œé¢
            "gl": "cn"
        }
        response = requests.get("https://serpapi.com/search", params=params, timeout=15)
        response.raise_for_status()
        search_data = response.json()
        return _parse_serpapi_results(search_data)
    except Exception as e:
        logging.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
        return []

def _parse_serpapi_results(data: dict) -> list:
    """è§£æ SerpAPI è¿”å›çš„åŸå§‹æ•°æ®"""
    results = []
    if "organic_results" in data:
        for item in data["organic_results"]:
            result = {
                "title": item.get("title"),
                "url": item.get("link"),
                "snippet": item.get("snippet"),
                "timestamp": item.get("date")  # è‹¥æœ‰æ—¶é—´ä¿¡æ¯ï¼Œå¯é€‰
            }
            results.append(result)
    # å¦‚æœæœ‰çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥æ·»åŠ ç½®é¡¶ï¼ˆå¯é€‰ï¼‰
    if "knowledge_graph" in data:
        kg = data["knowledge_graph"]
        results.insert(0, {
            "title": kg.get("title"),
            "url": kg.get("source", {}).get("link", ""),
            "snippet": kg.get("description"),
            "source": "knowledge_graph"
        })
    return results

def update_web_results(query: str, num_results: int = 5) -> list:
    """
    åŸºäº SerpAPI æœç´¢ç»“æœï¼Œå‘é‡åŒ–å¹¶å­˜å‚¨åˆ° ChromaDB
    ä¸ºç½‘ç»œç»“æœæ·»åŠ å…ƒæ•°æ®ï¼ŒID æ ¼å¼ä¸º "web_{index}"
    """
    results = serpapi_search(query, num_results)
    if not results:
        return []
    
    # åˆ é™¤æ—§çš„ç½‘ç»œæœç´¢ç»“æœï¼ˆä½¿ç”¨æ›´å¥å£®çš„æ–¹å¼ï¼‰
    try:
        # è·å–æ‰€æœ‰æ–‡æ¡£çš„å…ƒæ•°æ®
        collection_data = COLLECTION.get(include=['metadatas'])
        if collection_data and 'metadatas' in collection_data:
            # ä½¿ç”¨é›†åˆæ¨å¯¼ç”Ÿæˆè¦åˆ é™¤çš„IDåˆ—è¡¨
            web_ids = []
            for i, metadata in enumerate(collection_data['metadatas']):
                # å¦‚æœå…ƒæ•°æ®ä¸­çš„sourceå­—æ®µæ˜¯'web'ï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸€ä¸ªç½‘ç»œç»“æœ
                if metadata.get('source') == 'web' and i < len(collection_data['ids']):
                    web_ids.append(collection_data['ids'][i])
            
            # åˆ é™¤æ‰¾åˆ°çš„ç½‘ç»œç»“æœ
            if web_ids:
                COLLECTION.delete(ids=web_ids)
                logging.info(f"å·²åˆ é™¤ {len(web_ids)} æ¡æ—§çš„ç½‘ç»œæœç´¢ç»“æœ")
    except Exception as e:
        logging.warning(f"åˆ é™¤æ—§çš„ç½‘ç»œæœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
        # ç»§ç»­æ‰§è¡Œï¼Œä¸å½±å“æ–°ç»“æœæ·»åŠ 
    
    # å‡†å¤‡æ–°çš„ç½‘ç»œæœç´¢ç»“æœ
    docs = []
    metadatas = []
    ids = []
    for idx, res in enumerate(results):
        text = f"æ ‡é¢˜ï¼š{res.get('title', '')}\næ‘˜è¦ï¼š{res.get('snippet', '')}"
        docs.append(text)
        meta = {"source": "web", "url": res.get("url", ""), "title": res.get("title")}
        meta["content_hash"] = hashlib.md5(text.encode()).hexdigest()[:8]
        metadatas.append(meta)
        ids.append(f"web_{idx}")
    embeddings = EMBED_MODEL.encode(docs)
    COLLECTION.add(ids=ids, embeddings=embeddings.tolist(), documents=docs, metadatas=metadatas)
    return results

# æ£€æŸ¥æ˜¯å¦é…ç½®äº†SERPAPI_KEY
def check_serpapi_key():
    """æ£€æŸ¥æ˜¯å¦é…ç½®äº†SERPAPI_KEY"""
    return SERPAPI_KEY is not None and SERPAPI_KEY.strip() != ""

# æ·»åŠ æ–‡ä»¶å¤„ç†çŠ¶æ€è·Ÿè¸ª
class FileProcessor:
    def __init__(self):
        self.processed_files = {}  # å­˜å‚¨å·²å¤„ç†æ–‡ä»¶çš„çŠ¶æ€
        
    def clear_files(self):
        """æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶è®°å½•"""
        self.processed_files = {}
        
    def add_file(self, file_name):
        self.processed_files[file_name] = {
            'status': 'ç­‰å¾…å¤„ç†',
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'chunks': 0
        }
        
    def update_status(self, file_name, status, chunks=None):
        if file_name in self.processed_files:
            self.processed_files[file_name]['status'] = status
            if chunks is not None:
                self.processed_files[file_name]['chunks'] = chunks
                
    def get_file_list(self):
        return [
            f"ğŸ“„ {fname} | {info['status']}"
            for fname, info in self.processed_files.items()
        ]

file_processor = FileProcessor()

#########################################
# çŸ›ç›¾æ£€æµ‹å‡½æ•°
#########################################
def detect_conflicts(sources):
    """ç²¾å‡†çŸ›ç›¾æ£€æµ‹ç®—æ³•"""
    key_facts = {}
    for item in sources:
        facts = extract_facts(item['text'] if 'text' in item else item.get('excerpt', ''))
        for fact, value in facts.items():
            if fact in key_facts:
                if key_facts[fact] != value:
                    return True
            else:
                key_facts[fact] = value
    return False

def extract_facts(text):
    """ä»æ–‡æœ¬æå–å…³é”®äº‹å®ï¼ˆç¤ºä¾‹é€»è¾‘ï¼‰"""
    facts = {}
    # æå–æ•°å€¼å‹äº‹å®
    numbers = re.findall(r'\b\d{4}å¹´|\b\d+%', text)
    if numbers:
        facts['å…³é”®æ•°å€¼'] = numbers
    # æå–æŠ€æœ¯æœ¯è¯­
    if "äº§ä¸šå›¾è°±" in text:
        facts['æŠ€æœ¯æ–¹æ³•'] = list(set(re.findall(r'[A-Za-z]+æ¨¡å‹|[A-Z]{2,}ç®—æ³•', text)))
    return facts

def evaluate_source_credibility(source):
    """è¯„ä¼°æ¥æºå¯ä¿¡åº¦"""
    credibility_scores = {
        "gov.cn": 0.9,
        "edu.cn": 0.85,
        "weixin": 0.7,
        "zhihu": 0.6,
        "baidu": 0.5
    }
    
    url = source.get('url', '')
    if not url:
        return 0.5  # é»˜è®¤ä¸­ç­‰å¯ä¿¡åº¦
    
    domain_match = re.search(r'//([^/]+)', url)
    if not domain_match:
        return 0.5
    
    domain = domain_match.group(1)
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å·²çŸ¥åŸŸå
    for known_domain, score in credibility_scores.items():
        if known_domain in domain:
            return score
    
    return 0.5  # é»˜è®¤ä¸­ç­‰å¯ä¿¡åº¦

def extract_text(filepath):
    """æ”¹è¿›çš„PDFæ–‡æœ¬æå–æ–¹æ³•"""
    output = StringIO()
    with open(filepath, 'rb') as file:
        extract_text_to_fp(file, output)
    return output.getvalue()

def process_multiple_pdfs(files, progress=gr.Progress()):
    """å¤„ç†å¤šä¸ªPDFæ–‡ä»¶"""
    if not files:
        return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„PDFæ–‡ä»¶", []
    
    try:
        # æ¸…ç©ºå‘é‡æ•°æ®åº“
        progress(0.1, desc="æ¸…ç†å†å²æ•°æ®...")
        try:
            # ç›´æ¥è·å–æ‰€æœ‰IDï¼Œä¸ä½¿ç”¨includeå‚æ•°
            # ç”±äºChromaDBçš„é™åˆ¶ï¼Œæˆ‘ä»¬åªèƒ½è·å–æ‰€æœ‰æ•°æ®ï¼Œå¹¶ä»ä¸­æå–ID
            existing_data = COLLECTION.get()
            if existing_data and 'ids' in existing_data and existing_data['ids']:
                COLLECTION.delete(ids=existing_data['ids'])
                logging.info(f"æˆåŠŸæ¸…ç† {len(existing_data['ids'])} æ¡å†å²å‘é‡æ•°æ®")
            else:
                logging.info("æ²¡æœ‰æ‰¾åˆ°å†å²å‘é‡æ•°æ®éœ€è¦æ¸…ç†")
            # æ¸…ç©ºBM25ç´¢å¼•
            BM25_MANAGER.clear()
        except Exception as e:
            logging.error(f"æ¸…ç†å†å²æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return f"æ¸…ç†å†å²æ•°æ®å¤±è´¥: {str(e)}", []
        
        # æ¸…ç©ºæ–‡ä»¶å¤„ç†çŠ¶æ€
        file_processor.clear_files()
        
        total_files = len(files)
        processed_results = []
        total_chunks = 0
        
        for idx, file in enumerate(files, 1):
            try:
                file_name = os.path.basename(file.name)
                progress((idx-1)/total_files, desc=f"å¤„ç†æ–‡ä»¶ {idx}/{total_files}: {file_name}")
                
                # æ·»åŠ æ–‡ä»¶åˆ°å¤„ç†å™¨
                file_processor.add_file(file_name)
                
                # å¤„ç†å•ä¸ªæ–‡ä»¶
                text = extract_text(file.name)
                
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=400,        
                    chunk_overlap=40,     
                    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", "ï¼›", "ï¼š", " ", ""]  # æŒ‰è‡ªç„¶è¯­è¨€ç»“æ„åˆ†å‰²
                )
                chunks = text_splitter.split_text(text)
                
                if not chunks:
                    raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
                
                # ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†ç¬¦
                doc_id = f"doc_{int(time.time())}_{idx}"
                
                # ç”ŸæˆåµŒå…¥
                embeddings = EMBED_MODEL.encode(chunks)
                
                # å­˜å‚¨å‘é‡ï¼Œæ·»åŠ æ–‡æ¡£æºä¿¡æ¯
                ids = [f"{doc_id}_chunk_{i}" for i in range(len(chunks))]
                metadatas = [{"source": file_name, "doc_id": doc_id} for _ in chunks]
                
                COLLECTION.add(
                    ids=ids,
                    embeddings=embeddings.tolist(),
                    documents=chunks,
                    metadatas=metadatas
                )
                
                # æ›´æ–°å¤„ç†çŠ¶æ€
                total_chunks += len(chunks)
                file_processor.update_status(file_name, "å¤„ç†å®Œæˆ", len(chunks))
                processed_results.append(f"âœ… {file_name}: æˆåŠŸå¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—")
                
            except Exception as e:
                error_msg = str(e)
                logging.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {error_msg}")
                file_processor.update_status(file_name, f"å¤„ç†å¤±è´¥: {error_msg}")
                processed_results.append(f"âŒ {file_name}: å¤„ç†å¤±è´¥ - {error_msg}")
        
        # æ·»åŠ æ€»ç»“ä¿¡æ¯
        summary = f"\næ€»è®¡å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œ{total_chunks} ä¸ªæ–‡æœ¬å—"
        processed_results.append(summary)
        
        # æ›´æ–°BM25ç´¢å¼•
        progress(0.95, desc="æ„å»ºBM25æ£€ç´¢ç´¢å¼•...")
        update_bm25_index()
        
        # è·å–æ›´æ–°åçš„æ–‡ä»¶åˆ—è¡¨
        file_list = file_processor.get_file_list()
        
        return "\n".join(processed_results), file_list
        
    except Exception as e:
        error_msg = str(e)
        logging.error(f"æ•´ä½“å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}")
        return f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}", []

# æ–°å¢ï¼šäº¤å‰ç¼–ç å™¨é‡æ’åºå‡½æ•°
def rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k=5):
    """
    ä½¿ç”¨äº¤å‰ç¼–ç å™¨å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        top_k: è¿”å›ç»“æœæ•°é‡
        
    è¿”å›:
        é‡æ’åºåçš„ç»“æœåˆ—è¡¨ [(doc_id, {'content': doc, 'metadata': metadata, 'score': score}), ...]
    """
    if not docs:
        return []
        
    encoder = get_cross_encoder()
    if encoder is None:
        logging.warning("äº¤å‰ç¼–ç å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡é‡æ’åº")
        # è¿”å›åŸå§‹é¡ºåºï¼ˆæŒ‰ç´¢å¼•æ’åºï¼‰
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)}) 
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]
    
    # å‡†å¤‡äº¤å‰ç¼–ç å™¨è¾“å…¥
    cross_inputs = [[query, doc] for doc in docs]
    
    try:
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        scores = encoder.predict(cross_inputs)
        
        # ç»„åˆç»“æœ
        results = [
            (doc_id, {
                'content': doc, 
                'metadata': meta,
                'score': float(score)  # ç¡®ä¿æ˜¯PythonåŸç”Ÿç±»å‹
            }) 
            for doc_id, doc, meta, score in zip(doc_ids, docs, metadata_list, scores)
        ]
        
        # æŒ‰å¾—åˆ†æ’åº
        results = sorted(results, key=lambda x: x[1]['score'], reverse=True)
        
        # è¿”å›å‰Kä¸ªç»“æœ
        return results[:top_k]
    except Exception as e:
        logging.error(f"äº¤å‰ç¼–ç å™¨é‡æ’åºå¤±è´¥: {str(e)}")
        # å‡ºé”™æ—¶è¿”å›åŸå§‹é¡ºåº
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)}) 
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]

# æ–°å¢ï¼šLLMç›¸å…³æ€§è¯„åˆ†å‡½æ•°
@lru_cache(maxsize=32)
def get_llm_relevance_score(query, doc):
    """
    ä½¿ç”¨LLMå¯¹æŸ¥è¯¢å’Œæ–‡æ¡£çš„ç›¸å…³æ€§è¿›è¡Œè¯„åˆ†ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        doc: æ–‡æ¡£å†…å®¹
        
    è¿”å›:
        ç›¸å…³æ€§å¾—åˆ† (0-10)
    """
    try:
        # æ„å»ºè¯„åˆ†æç¤ºè¯
        prompt = f"""ç»™å®šä»¥ä¸‹æŸ¥è¯¢å’Œæ–‡æ¡£ç‰‡æ®µï¼Œè¯„ä¼°å®ƒä»¬çš„ç›¸å…³æ€§ã€‚
        è¯„åˆ†æ ‡å‡†ï¼š0åˆ†è¡¨ç¤ºå®Œå…¨ä¸ç›¸å…³ï¼Œ10åˆ†è¡¨ç¤ºé«˜åº¦ç›¸å…³ã€‚
        åªéœ€è¿”å›ä¸€ä¸ª0-10ä¹‹é—´çš„æ•´æ•°åˆ†æ•°ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–è§£é‡Šã€‚
        
        æŸ¥è¯¢: {query}
        
        æ–‡æ¡£ç‰‡æ®µ: {doc}
        
        ç›¸å…³æ€§åˆ†æ•°(0-10):"""
        
        # è°ƒç”¨æœ¬åœ°LLM
        response = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "deepseek-r1:1.5b",  # ä½¿ç”¨è¾ƒå°æ¨¡å‹è¿›è¡Œè¯„åˆ†
                "prompt": prompt,
                "stream": False
            },
            timeout=30
        )
        
        # æå–å¾—åˆ†
        result = response.json().get("response", "").strip()
        
        # å°è¯•è§£æä¸ºæ•°å­—
        try:
            score = float(result)
            # ç¡®ä¿åˆ†æ•°åœ¨0-10èŒƒå›´å†…
            score = max(0, min(10, score))
            return score
        except ValueError:
            # å¦‚æœæ— æ³•è§£æä¸ºæ•°å­—ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—
            match = re.search(r'\b([0-9]|10)\b', result)
            if match:
                return float(match.group(1))
            else:
                # é»˜è®¤è¿”å›ä¸­ç­‰ç›¸å…³æ€§
                return 5.0
                
    except Exception as e:
        logging.error(f"LLMè¯„åˆ†å¤±è´¥: {str(e)}")
        # é»˜è®¤è¿”å›ä¸­ç­‰ç›¸å…³æ€§
        return 5.0

def rerank_with_llm(query, docs, doc_ids, metadata_list, top_k=5):
    """
    ä½¿ç”¨LLMå¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        top_k: è¿”å›ç»“æœæ•°é‡
    
    è¿”å›:
        é‡æ’åºåçš„ç»“æœåˆ—è¡¨
    """
    if not docs:
        return []
    
    results = []
    
    # å¯¹æ¯ä¸ªæ–‡æ¡£è¿›è¡Œè¯„åˆ†
    for doc_id, doc, meta in zip(doc_ids, docs, metadata_list):
        # è·å–LLMè¯„åˆ†
        score = get_llm_relevance_score(query, doc)
        
        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        results.append((doc_id, {
            'content': doc, 
            'metadata': meta,
            'score': score / 10.0  # å½’ä¸€åŒ–åˆ°0-1
        }))
    
    # æŒ‰å¾—åˆ†æ’åº
    results = sorted(results, key=lambda x: x[1]['score'], reverse=True)
    
    # è¿”å›å‰Kä¸ªç»“æœ
    return results[:top_k]

# æ–°å¢ï¼šé€šç”¨é‡æ’åºå‡½æ•°
def rerank_results(query, docs, doc_ids, metadata_list, method=None, top_k=5):
    """
    å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        method: é‡æ’åºæ–¹æ³• ("cross_encoder", "llm" æˆ– None)
        top_k: è¿”å›ç»“æœæ•°é‡
        
    è¿”å›:
        é‡æ’åºåçš„ç»“æœ
    """
    # å¦‚æœæœªæŒ‡å®šæ–¹æ³•ï¼Œä½¿ç”¨å…¨å±€é…ç½®
    if method is None:
        method = RERANK_METHOD
    
    # æ ¹æ®æ–¹æ³•é€‰æ‹©é‡æ’åºå‡½æ•°
    if method == "llm":
        return rerank_with_llm(query, docs, doc_ids, metadata_list, top_k)
    elif method == "cross_encoder":
        return rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k)
    else:
        # é»˜è®¤ä¸è¿›è¡Œé‡æ’åºï¼ŒæŒ‰åŸå§‹é¡ºåºè¿”å›
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)}) 
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]

def stream_answer(question, enable_web_search=False, model_choice="ollama", progress=gr.Progress()):
    """æ”¹è¿›çš„æµå¼é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢ã€æ··åˆæ£€ç´¢å’Œé‡æ’åºï¼Œä»¥åŠå¤šç§æ¨¡å‹é€‰æ‹©"""
    try:
        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦ä¸ºç©º
        try:
            collection_data = COLLECTION.get(include=["documents"])
            if not collection_data or not collection_data.get("documents") or len(collection_data.get("documents", [])) == 0:
                if not enable_web_search:
                    yield "âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚", "é‡åˆ°é”™è¯¯"
                    return
                else:
                    logging.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œå°†ä»…ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœ")
        except Exception as e:
            if not enable_web_search:
                yield f"âš ï¸ æ£€æŸ¥çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}ï¼Œè¯·ç¡®ä¿å·²ä¸Šä¼ æ–‡æ¡£ã€‚", "é‡åˆ°é”™è¯¯"
                return
            logging.error(f"æ£€æŸ¥çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}")
        
        progress(0.3, desc="æ‰§è¡Œé€’å½’æ£€ç´¢...")
        # ä½¿ç”¨é€’å½’æ£€ç´¢è·å–æ›´å…¨é¢çš„ç­”æ¡ˆä¸Šä¸‹æ–‡
        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(
            initial_query=question,
            max_iterations=3,
            enable_web_search=enable_web_search,
            model_choice=model_choice
        )
        
        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        sources_for_conflict_detection = []
        
        # ä½¿ç”¨æ£€ç´¢åˆ°çš„ç»“æœæ„å»ºä¸Šä¸‹æ–‡
        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):
            source_type = metadata.get('source', 'æœ¬åœ°æ–‡æ¡£')
            
            source_item = {
                'text': doc,
                'type': source_type
            }
            
            if source_type == 'web':
                url = metadata.get('url', 'æœªçŸ¥URL')
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                context_with_sources.append(f"[ç½‘ç»œæ¥æº: {title}] (URL: {url})\n{doc}")
                source_item['url'] = url
                source_item['title'] = title
            else:
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                context_with_sources.append(f"[æœ¬åœ°æ–‡æ¡£: {source}]\n{doc}")
                source_item['source'] = source
            
            sources_for_conflict_detection.append(source_item)
        
        # æ£€æµ‹çŸ›ç›¾
        conflict_detected = detect_conflicts(sources_for_conflict_detection)
        
        # è·å–å¯ä¿¡æº
        if conflict_detected:
            credible_sources = [s for s in sources_for_conflict_detection 
                               if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]
        
        context = "\n\n".join(context_with_sources)
        
        # æ·»åŠ æ—¶é—´æ•æ„Ÿæ£€æµ‹
        time_sensitive = any(word in question for word in ["æœ€æ–°", "ä»Šå¹´", "å½“å‰", "æœ€è¿‘", "åˆšåˆš"])
        
        # æ”¹è¿›æç¤ºè¯æ¨¡æ¿ï¼Œæé«˜å›ç­”è´¨é‡
        prompt_template = """ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œä½ éœ€è¦åŸºäºä»¥ä¸‹{context_type}å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æä¾›çš„å‚è€ƒå†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·éµå¾ªä»¥ä¸‹å›ç­”åŸåˆ™ï¼š
1. ä»…åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†
2. å¦‚æœå‚è€ƒå†…å®¹ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·å¦è¯šå‘ŠçŸ¥ä½ æ— æ³•å›ç­”
3. å›ç­”åº”è¯¥å…¨é¢ã€å‡†ç¡®ã€æœ‰æ¡ç†ï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„æ®µè½å’Œç»“æ„
4. è¯·ç”¨ä¸­æ–‡å›ç­”
5. åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æº{time_instruction}{conflict_instruction}

è¯·ç°åœ¨å¼€å§‹å›ç­”ï¼š"""
        
        prompt = prompt_template.format(
            context_type="æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œæœç´¢ç»“æœ" if enable_web_search else "æœ¬åœ°æ–‡æ¡£",
            context=context,
            question=question,
            time_instruction="ï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„ä¿¡æ¯" if time_sensitive and enable_web_search else "",
            conflict_instruction="ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºä¸åŒæ¥æºçš„å·®å¼‚" if conflict_detected else ""
        )
        
        progress(0.7, desc="ç”Ÿæˆå›ç­”...")
        full_answer = ""
        
        # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„API
        if model_choice == "siliconflow":
            # å¯¹äºSiliconFlow APIï¼Œä¸æ”¯æŒæµå¼å“åº”ï¼Œæ‰€ä»¥ä¸€æ¬¡æ€§è·å–
            progress(0.8, desc="é€šè¿‡SiliconFlow APIç”Ÿæˆå›ç­”...")
            full_answer = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)
            
            # å¤„ç†æ€ç»´é“¾
            if "<think>" in full_answer and "</think>" in full_answer:
                processed_answer = process_thinking_content(full_answer)
            else:
                processed_answer = full_answer
                
            yield processed_answer, "å®Œæˆ!"
        else:
            # ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹çš„æµå¼å“åº”
            response = session.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "deepseek-r1:1.5b",
                    "prompt": prompt,
                    "stream": True
                },
                timeout=120,
                stream=True
            )
            
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode()).get("response", "")
                    full_answer += chunk
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„æ€ç»´é“¾æ ‡ç­¾å¯ä»¥å¤„ç†
                    if "<think>" in full_answer and "</think>" in full_answer:
                        # éœ€è¦ç¡®ä¿å®Œæ•´æ”¶é›†ä¸€ä¸ªæ€ç»´é“¾ç‰‡æ®µåå†æ˜¾ç¤º
                        processed_answer = process_thinking_content(full_answer)
                    else:
                        processed_answer = full_answer
                    
                    yield processed_answer, "ç”Ÿæˆå›ç­”ä¸­..."
                    
            # å¤„ç†æœ€ç»ˆè¾“å‡ºï¼Œç¡®ä¿åº”ç”¨æ€ç»´é“¾å¤„ç†
            final_answer = process_thinking_content(full_answer)
            yield final_answer, "å®Œæˆ!"
        
    except Exception as e:
        yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}", "é‡åˆ°é”™è¯¯"

def query_answer(question, enable_web_search=False, model_choice="ollama", progress=gr.Progress()):
    """é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢ã€æ··åˆæ£€ç´¢å’Œé‡æ’åºï¼Œä»¥åŠå¤šç§æ¨¡å‹é€‰æ‹©"""
    try:
        logging.info(f"æ”¶åˆ°é—®é¢˜ï¼š{question}ï¼Œè”ç½‘çŠ¶æ€ï¼š{enable_web_search}ï¼Œæ¨¡å‹é€‰æ‹©ï¼š{model_choice}")
        
        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦ä¸ºç©º
        try:
            collection_data = COLLECTION.get(include=["documents"])
            if not collection_data or not collection_data.get("documents") or len(collection_data.get("documents", [])) == 0:
                if not enable_web_search:
                    return "âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚"
                else:
                    logging.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œå°†ä»…ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœ")
        except Exception as e:
            if not enable_web_search:
                return f"âš ï¸ æ£€æŸ¥çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}ï¼Œè¯·ç¡®ä¿å·²ä¸Šä¼ æ–‡æ¡£ã€‚"
            logging.error(f"æ£€æŸ¥çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}")
        
        progress(0.3, desc="æ‰§è¡Œé€’å½’æ£€ç´¢...")
        # ä½¿ç”¨é€’å½’æ£€ç´¢è·å–æ›´å…¨é¢çš„ç­”æ¡ˆä¸Šä¸‹æ–‡
        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(
            initial_query=question,
            max_iterations=3,
            enable_web_search=enable_web_search,
            model_choice=model_choice
        )
        
        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        sources_for_conflict_detection = []
        
        # ä½¿ç”¨æ£€ç´¢åˆ°çš„ç»“æœæ„å»ºä¸Šä¸‹æ–‡
        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):
            source_type = metadata.get('source', 'æœ¬åœ°æ–‡æ¡£')
            
            source_item = {
                'text': doc,
                'type': source_type
            }
            
            if source_type == 'web':
                url = metadata.get('url', 'æœªçŸ¥URL')
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                context_with_sources.append(f"[ç½‘ç»œæ¥æº: {title}] (URL: {url})\n{doc}")
                source_item['url'] = url
                source_item['title'] = title
            else:
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                context_with_sources.append(f"[æœ¬åœ°æ–‡æ¡£: {source}]\n{doc}")
                source_item['source'] = source
            
            sources_for_conflict_detection.append(source_item)
        
        # æ£€æµ‹çŸ›ç›¾
        conflict_detected = detect_conflicts(sources_for_conflict_detection)
        
        # è·å–å¯ä¿¡æº
        if conflict_detected:
            credible_sources = [s for s in sources_for_conflict_detection 
                              if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]
        
        context = "\n\n".join(context_with_sources)
        
        # æ·»åŠ æ—¶é—´æ•æ„Ÿæ£€æµ‹
        time_sensitive = any(word in question for word in ["æœ€æ–°", "ä»Šå¹´", "å½“å‰", "æœ€è¿‘", "åˆšåˆš"])
        
        # æ”¹è¿›æç¤ºè¯æ¨¡æ¿ï¼Œæé«˜å›ç­”è´¨é‡
        prompt_template = """ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œä½ éœ€è¦åŸºäºä»¥ä¸‹{context_type}å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æä¾›çš„å‚è€ƒå†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·éµå¾ªä»¥ä¸‹å›ç­”åŸåˆ™ï¼š
1. ä»…åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†
2. å¦‚æœå‚è€ƒå†…å®¹ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·å¦è¯šå‘ŠçŸ¥ä½ æ— æ³•å›ç­”
3. å›ç­”åº”è¯¥å…¨é¢ã€å‡†ç¡®ã€æœ‰æ¡ç†ï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„æ®µè½å’Œç»“æ„
4. è¯·ç”¨ä¸­æ–‡å›ç­”
5. åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æº{time_instruction}{conflict_instruction}

è¯·ç°åœ¨å¼€å§‹å›ç­”ï¼š"""
        
        prompt = prompt_template.format(
            context_type="æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œæœç´¢ç»“æœ" if enable_web_search else "æœ¬åœ°æ–‡æ¡£",
            context=context,
            question=question,
            time_instruction="ï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„ä¿¡æ¯" if time_sensitive and enable_web_search else "",
            conflict_instruction="ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºä¸åŒæ¥æºçš„å·®å¼‚" if conflict_detected else ""
        )
        
        progress(0.8, desc="ç”Ÿæˆå›ç­”...")
        
        # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„API
        if model_choice == "siliconflow":
            # ä½¿ç”¨SiliconFlow API
            result = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)
            
            # å¤„ç†æ€ç»´é“¾
            processed_result = process_thinking_content(result)
            return processed_result
        else:
            # ä½¿ç”¨æœ¬åœ°Ollama
            response = session.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "deepseek-r1:7b",
                    "prompt": prompt,
                    "stream": False
                },
                timeout=120,  # å»¶é•¿åˆ°2åˆ†é’Ÿ
                headers={'Connection': 'close'}  # æ·»åŠ è¿æ¥å¤´
            )
            response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 
            
            progress(1.0, desc="å®Œæˆ!")
            # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²å¹¶å¤„ç†ç©ºå€¼
            result = response.json()
            return process_thinking_content(str(result.get("response", "æœªè·å–åˆ°æœ‰æ•ˆå›ç­”")))
            
    except json.JSONDecodeError:
        return "å“åº”è§£æå¤±è´¥ï¼Œè¯·é‡è¯•"
    except KeyError:
        return "å“åº”æ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡"
    except Exception as e:
        progress(1.0, desc="é‡åˆ°é”™è¯¯")  # ç¡®ä¿è¿›åº¦æ¡å®Œæˆ
        return f"ç³»ç»Ÿé”™è¯¯: {str(e)}"

def process_thinking_content(text):
    """å¤„ç†åŒ…å«<think>æ ‡ç­¾çš„å†…å®¹ï¼Œå°†å…¶è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ–‡æœ¬
    if text is None:
        return ""
    
    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    if not isinstance(text, str):
        try:
            processed_text = str(text)
        except:
            return "æ— æ³•å¤„ç†çš„å†…å®¹æ ¼å¼"
    else:
        processed_text = text
    
    # å¤„ç†æ€ç»´é“¾æ ‡ç­¾
    try:
        while "<think>" in processed_text and "</think>" in processed_text:
            start_idx = processed_text.find("<think>")
            end_idx = processed_text.find("</think>")
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                thinking_content = processed_text[start_idx + 7:end_idx]
                before_think = processed_text[:start_idx]
                after_think = processed_text[end_idx + 8:]
                
                # ä½¿ç”¨å¯æŠ˜å è¯¦æƒ…æ¡†æ˜¾ç¤ºæ€ç»´é“¾
                processed_text = before_think + "\n\n<details>\n<summary>æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n" + thinking_content + "\n\n</details>\n\n" + after_think
        
        # å¤„ç†å…¶ä»–HTMLæ ‡ç­¾ï¼Œä½†ä¿ç•™detailså’Œsummaryæ ‡ç­¾
        processed_html = []
        i = 0
        while i < len(processed_text):
            if processed_text[i:i+8] == "<details" or processed_text[i:i+9] == "</details" or \
               processed_text[i:i+8] == "<summary" or processed_text[i:i+9] == "</summary":
                # ä¿ç•™è¿™äº›æ ‡ç­¾
                tag_end = processed_text.find(">", i)
                if tag_end != -1:
                    processed_html.append(processed_text[i:tag_end+1])
                    i = tag_end + 1
                    continue
            
            if processed_text[i] == "<":
                processed_html.append("&lt;")
            elif processed_text[i] == ">":
                processed_html.append("&gt;")
            else:
                processed_html.append(processed_text[i])
            i += 1
        
        processed_text = "".join(processed_html)
    except Exception as e:
        logging.error(f"å¤„ç†æ€ç»´é“¾å†…å®¹æ—¶å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶è‡³å°‘è¿”å›åŸå§‹æ–‡æœ¬ï¼Œä½†ç¡®ä¿å®‰å…¨å¤„ç†HTMLæ ‡ç­¾
        try:
            return text.replace("<", "&lt;").replace(">", "&gt;")
        except:
            return "å¤„ç†å†…å®¹æ—¶å‡ºé”™"
    
    return processed_text

def call_siliconflow_api(prompt, temperature=0.7, max_tokens=1024):
    """
    è°ƒç”¨SiliconFlow APIè·å–å›ç­”
    
    Args:
        prompt: æç¤ºè¯
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        
    Returns:
        ç”Ÿæˆçš„å›ç­”æ–‡æœ¬å’Œæ€ç»´é“¾å†…å®¹
    """
    try:
        payload = {
            "model": "Pro/deepseek-ai/DeepSeek-R1",
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "stream": False,
            "max_tokens": max_tokens,
            "stop": None,
            "temperature": temperature,
            "top_p": 0.7,
            "top_k": 50,
            "frequency_penalty": 0.5,
            "n": 1,
            "response_format": {"type": "text"}
        }
        
        headers = {
            "Authorization": f"Bearer {SILICONFLOW_API_KEY}",
            "Content-Type": "application/json"
        }
        
        response = requests.post(
            SILICONFLOW_API_URL,
            json=payload,
            headers=headers,
            timeout=60  # å»¶é•¿è¶…æ—¶æ—¶é—´
        )
        
        response.raise_for_status()
        result = response.json()
        
        # æå–å›ç­”å†…å®¹å’Œæ€ç»´é“¾
        if "choices" in result and len(result["choices"]) > 0:
            message = result["choices"][0]["message"]
            content = message.get("content", "")
            reasoning = message.get("reasoning_content", "")
            
            # å¦‚æœæœ‰æ€ç»´é“¾ï¼Œåˆ™æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä»¥ä¾¿å‰ç«¯å¤„ç†
            if reasoning:
                # æ·»åŠ æ€ç»´é“¾æ ‡è®°
                full_response = f"{content}<think>{reasoning}</think>"
                return full_response
            else:
                return content
        else:
            return "APIè¿”å›ç»“æœæ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥"
            
    except requests.exceptions.RequestException as e:
        logging.error(f"è°ƒç”¨SiliconFlow APIæ—¶å‡ºé”™: {str(e)}")
        return f"è°ƒç”¨APIæ—¶å‡ºé”™: {str(e)}"
    except json.JSONDecodeError:
        logging.error("SiliconFlow APIè¿”å›éJSONå“åº”")
        return "APIå“åº”è§£æå¤±è´¥"
    except Exception as e:
        logging.error(f"è°ƒç”¨SiliconFlow APIæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        return f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"

def hybrid_merge(semantic_results, bm25_results, alpha=0.7):
    """
    åˆå¹¶è¯­ä¹‰æœç´¢å’ŒBM25æœç´¢ç»“æœ
    
    å‚æ•°:
        semantic_results: å‘é‡æ£€ç´¢ç»“æœ
        bm25_results: BM25æ£€ç´¢ç»“æœ
        alpha: è¯­ä¹‰æœç´¢æƒé‡ (0-1)
        
    è¿”å›:
        åˆå¹¶åçš„ç»“æœåˆ—è¡¨
    """
    # åˆ›å»ºIDåˆ°å¾—åˆ†å’Œå†…å®¹çš„æ˜ å°„
    merged_dict = {}
    
    # å¤„ç†è¯­ä¹‰æœç´¢ç»“æœ
    if (semantic_results and 'documents' in semantic_results and 'metadatas' in semantic_results and 
        semantic_results['documents'] and semantic_results['metadatas'] and 
        len(semantic_results['documents']) > 0 and len(semantic_results['documents'][0]) > 0):
        
        # ç”Ÿæˆæ–‡æ¡£IDï¼Œä½¿ç”¨å…ƒæ•°æ®ä¸­çš„doc_idå­—æ®µ
        for i, (doc, meta) in enumerate(zip(semantic_results['documents'][0], semantic_results['metadatas'][0])):
            # ä»å…ƒæ•°æ®ä¸­æå–doc_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç´¢å¼•ä½ç½®
            doc_id = f"{meta.get('doc_id', 'doc')}_{i}"
            
            score = 1.0 - (i / max(1, len(semantic_results['documents'][0])))  # å½’ä¸€åŒ–åˆ†æ•°ï¼Œé¿å…é™¤ä»¥é›¶
            merged_dict[doc_id] = {
                'score': alpha * score, 
                'content': doc,
                'metadata': meta
            }
    
    # å¤„ç†BM25ç»“æœ
    if not bm25_results:
        # å¦‚æœBM25ç»“æœä¸ºç©ºï¼Œç›´æ¥è¿”å›è¯­ä¹‰ç»“æœ
        return sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)
        
    max_bm25_score = max([r['score'] for r in bm25_results] or [1.0])
    for result in bm25_results:
        doc_id = result['id']
        # å½’ä¸€åŒ–BM25åˆ†æ•°
        normalized_score = result['score'] / max_bm25_score if max_bm25_score > 0 else 0
        
        if doc_id in merged_dict:
            # å¦‚æœå·²å­˜åœ¨ï¼Œåˆå¹¶åˆ†æ•°
            merged_dict[doc_id]['score'] += (1 - alpha) * normalized_score
        else:
            # è·å–å…ƒæ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            metadata = {}
            try:
                metadata_result = COLLECTION.get(ids=[doc_id], include=["metadatas"])
                if metadata_result and metadata_result['metadatas']:
                    metadata = metadata_result['metadatas'][0]
            except Exception as e:
                logging.warning(f"è·å–æ–‡æ¡£å…ƒæ•°æ®å¤±è´¥: {str(e)}")
                
            merged_dict[doc_id] = {
                'score': (1 - alpha) * normalized_score,
                'content': result['content'],
                'metadata': metadata
            }
    
    # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›ç»“æœ
    merged_results = sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)
    
    return merged_results

# æ–°å¢ï¼šæ›´æ–°æœ¬åœ°æ–‡æ¡£çš„BM25ç´¢å¼•
def update_bm25_index():
    """æ›´æ–°BM25ç´¢å¼•ï¼Œä»ChromaDBåŠ è½½æ‰€æœ‰æ–‡æ¡£"""
    try:
        # è·å–æ‰€æœ‰æ–‡æ¡£
        all_docs = COLLECTION.get(include=['documents', 'metadatas'])
        
        if not all_docs or not all_docs['documents']:
            logging.warning("æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡æ¡£")
            BM25_MANAGER.clear()
            return False
            
        # æ„å»ºBM25ç´¢å¼•
        # ä½¿ç”¨metadatasä¸­çš„doc_idåˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
        doc_ids = [f"{meta.get('doc_id', 'unknown')}_{idx}" for idx, meta in enumerate(all_docs['metadatas'])]
        BM25_MANAGER.build_index(all_docs['documents'], doc_ids)
        logging.info(f"BM25ç´¢å¼•æ›´æ–°å®Œæˆï¼Œå…±ç´¢å¼• {len(doc_ids)} ä¸ªæ–‡æ¡£")
        return True
    except Exception as e:
        logging.error(f"æ›´æ–°BM25ç´¢å¼•å¤±è´¥: {str(e)}")
        return False

# æ–°å¢å‡½æ•°ï¼šè·å–ç³»ç»Ÿä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
def get_system_models_info():
    """è¿”å›ç³»ç»Ÿä½¿ç”¨çš„å„ç§æ¨¡å‹ä¿¡æ¯"""
    models_info = {
        "åµŒå…¥æ¨¡å‹": "all-MiniLM-L6-v2",
        "åˆ†å—æ–¹æ³•": "RecursiveCharacterTextSplitter (chunk_size=800, overlap=150)",
        "æ£€ç´¢æ–¹æ³•": "å‘é‡æ£€ç´¢ + BM25æ··åˆæ£€ç´¢ (Î±=0.7)",
        "é‡æ’åºæ¨¡å‹": "äº¤å‰ç¼–ç å™¨ (sentence-transformers/distiluse-base-multilingual-cased-v2)",
        "ç”Ÿæˆæ¨¡å‹": "deepseek-r1 (7B/1.5B)",
        "åˆ†è¯å·¥å…·": "jieba (ä¸­æ–‡åˆ†è¯)"
    }
    return models_info

# æ–°å¢å‡½æ•°ï¼šè·å–æ–‡æ¡£åˆ†å—å¯è§†åŒ–æ•°æ®
def get_document_chunks(progress=gr.Progress()):
    """è·å–æ–‡æ¡£åˆ†å—ç»“æœç”¨äºå¯è§†åŒ–"""
    try:
        progress(0.1, desc="æ­£åœ¨æŸ¥è¯¢æ•°æ®åº“...")
        # è·å–æ‰€æœ‰æ–‡æ¡£å—
        all_docs = COLLECTION.get(include=['documents', 'metadatas'])
        
        if not all_docs or not all_docs['documents'] or len(all_docs['documents']) == 0:
            return [], "æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£ï¼Œè¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£ã€‚"
            
        progress(0.5, desc="æ­£åœ¨ç»„ç»‡åˆ†å—æ•°æ®...")
        
        # æŒ‰æ–‡æ¡£æºåˆ†ç»„
        doc_groups = {}
        for doc, meta in zip(all_docs['documents'], all_docs['metadatas']):
            source = meta.get('source', 'æœªçŸ¥æ¥æº')
            if source not in doc_groups:
                doc_groups[source] = []
            
            # æå–æ–‡æ¡£IDå’Œåˆ†å—ä¿¡æ¯
            doc_id = meta.get('doc_id', 'æœªçŸ¥ID')
            chunk_info = {
                "doc_id": doc_id,
                "content": doc[:200] + "..." if len(doc) > 200 else doc,  # æˆªå–å‰200ä¸ªå­—ç¬¦
                "full_content": doc,
                "token_count": len(list(jieba.cut(doc))),  # åˆ†è¯åçš„tokenæ•°é‡
                "char_count": len(doc)
            }
            doc_groups[source].append(chunk_info)
        
        # æ•´ç†ä¸ºè¡¨æ ¼æ•°æ®ï¼ˆé‡‡ç”¨äºŒç»´åˆ—è¡¨æ ¼å¼ï¼Œè€Œä¸æ˜¯å­—å…¸åˆ—è¡¨ï¼‰
        result_dicts = []  # ä¿å­˜åŸå§‹å­—å…¸æ ¼å¼ç”¨äºæ˜¾ç¤ºè¯¦æƒ…
        result_lists = []  # ä¿å­˜åˆ—è¡¨æ ¼å¼ç”¨äºDataframeæ˜¾ç¤º
        
        for source, chunks in doc_groups.items():
            for i, chunk in enumerate(chunks):
                # æ„å»ºå­—å…¸æ ¼å¼æ•°æ®ï¼ˆç”¨äºä¿å­˜å®Œæ•´ä¿¡æ¯ï¼‰
                result_dict = {
                    "æ¥æº": source,
                    "åºå·": f"{i+1}/{len(chunks)}",
                    "å­—ç¬¦æ•°": chunk["char_count"],
                    "åˆ†è¯æ•°": chunk["token_count"],
                    "å†…å®¹é¢„è§ˆ": chunk["content"][:200] + "..." if len(chunk["content"]) > 200 else chunk["content"],
                    "å®Œæ•´å†…å®¹": chunk["full_content"]
                }
                result_dicts.append(result_dict)
                
                # æ„å»ºåˆ—è¡¨æ ¼å¼æ•°æ®ï¼ˆç”¨äºDataframeæ˜¾ç¤ºï¼‰
                result_lists.append([
                    source,
                    f"{i+1}/{len(chunks)}",
                    chunk["char_count"],
                    chunk["token_count"],
                    chunk["content"][:200] + "..." if len(chunk["content"]) > 200 else chunk["content"]
                ])
        
        progress(1.0, desc="æ•°æ®åŠ è½½å®Œæˆ!")
        
        # ä¿å­˜åŸå§‹å­—å…¸æ•°æ®ä»¥ä¾¿åœ¨è¯¦æƒ…æŸ¥çœ‹æ—¶ä½¿ç”¨
        global chunk_data_cache
        chunk_data_cache = result_dicts
        
        summary = f"æ€»è®¡ {len(result_lists)} ä¸ªæ–‡æœ¬å—ï¼Œæ¥è‡ª {len(doc_groups)} ä¸ªä¸åŒæ¥æºã€‚"
        
        return result_lists, summary
    except Exception as e:
        return [], f"è·å–åˆ†å—æ•°æ®å¤±è´¥: {str(e)}"

# æ·»åŠ å…¨å±€ç¼“å­˜å˜é‡
chunk_data_cache = []

# æ–°å¢å‡½æ•°ï¼šæ˜¾ç¤ºåˆ†å—è¯¦æƒ…
def show_chunk_details(evt: gr.SelectData, chunks):
    """æ˜¾ç¤ºé€‰ä¸­åˆ†å—çš„è¯¦ç»†å†…å®¹"""
    try:
        if evt.index[0] < len(chunk_data_cache):
            selected_chunk = chunk_data_cache[evt.index[0]]
            return selected_chunk.get("å®Œæ•´å†…å®¹", "å†…å®¹åŠ è½½å¤±è´¥")
        return "æœªæ‰¾åˆ°é€‰ä¸­çš„åˆ†å—"
    except Exception as e:
        return f"åŠ è½½åˆ†å—è¯¦æƒ…å¤±è´¥: {str(e)}"

# ä¿®æ”¹å¸ƒå±€éƒ¨åˆ†ï¼Œæ·»åŠ ä¸€ä¸ªæ–°çš„æ ‡ç­¾é¡µ
with gr.Blocks(
    title="æœ¬åœ°RAGé—®ç­”ç³»ç»Ÿ",
    css="""
    /* å…¨å±€ä¸»é¢˜å˜é‡ */
    :root[data-theme="light"] {
        --text-color: #2c3e50;
        --bg-color: #ffffff;
        --panel-bg: #f8f9fa;
        --border-color: #e9ecef;
        --success-color: #4CAF50;
        --error-color: #f44336;
        --primary-color: #2196F3;
        --secondary-bg: #ffffff;
        --hover-color: #e9ecef;
        --chat-user-bg: #e3f2fd;
        --chat-assistant-bg: #f5f5f5;
    }

    :root[data-theme="dark"] {
        --text-color: #e0e0e0;
        --bg-color: #1a1a1a;
        --panel-bg: #2d2d2d;
        --border-color: #404040;
        --success-color: #81c784;
        --error-color: #e57373;
        --primary-color: #64b5f6;
        --secondary-bg: #2d2d2d;
        --hover-color: #404040;
        --chat-user-bg: #1e3a5f;
        --chat-assistant-bg: #2d2d2d;
    }

    /* å…¨å±€æ ·å¼ */
    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
        margin: 0;
        padding: 0;
        overflow-x: hidden;
        width: 100vw;
        height: 100vh;
    }

    .gradio-container {
        max-width: 100% !important;
        width: 100% !important;
        margin: 0 !important;
        padding: 0 1% !important;
        color: var(--text-color);
        background-color: var(--bg-color);
        min-height: 100vh;
    }
    
    /* ç¡®ä¿æ ‡ç­¾å†…å®¹æ’‘æ»¡ */
    .tabs.svelte-710i53 {
        margin: 0 !important;
        padding: 0 !important;
        width: 100% !important;
    }

    /* ä¸»é¢˜åˆ‡æ¢æŒ‰é’® */
    .theme-toggle {
        position: fixed;
        top: 20px;
        right: 20px;
        z-index: 1000;
        padding: 8px 16px;
        border-radius: 20px;
        border: 1px solid var(--border-color);
        background: var(--panel-bg);
        color: var(--text-color);
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 14px;
        display: flex;
        align-items: center;
        gap: 8px;
    }

    .theme-toggle:hover {
        background: var(--hover-color);
    }

    /* é¢æ¿æ ·å¼ */
    .left-panel {
        padding-right: 20px;
        border-right: 1px solid var(--border-color);
        background: var(--bg-color);
        width: 100%;
    }

    .right-panel {
        height: 100vh;
        background: var(--bg-color);
        width: 100%;
    }

    /* æ–‡ä»¶åˆ—è¡¨æ ·å¼ */
    .file-list {
        margin-top: 10px;
        padding: 12px;
        background: var(--panel-bg);
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.6;
        border: 1px solid var(--border-color);
    }

    /* ç­”æ¡ˆæ¡†æ ·å¼ */
    .answer-box {
        min-height: 500px !important;
        background: var(--panel-bg);
        border-radius: 8px;
        padding: 16px;
        font-size: 15px;
        line-height: 1.6;
        border: 1px solid var(--border-color);
    }

    /* è¾“å…¥æ¡†æ ·å¼ */
    textarea {
        background: var(--panel-bg) !important;
        color: var(--text-color) !important;
        border: 1px solid var(--border-color) !important;
        border-radius: 8px !important;
        padding: 12px !important;
        font-size: 14px !important;
    }

    /* æŒ‰é’®æ ·å¼ */
    button.primary {
        background: var(--primary-color) !important;
        color: white !important;
        border-radius: 8px !important;
        padding: 8px 16px !important;
        font-weight: 500 !important;
        transition: all 0.3s ease !important;
    }

    button.primary:hover {
        opacity: 0.9;
        transform: translateY(-1px);
    }

    /* æ ‡é¢˜å’Œæ–‡æœ¬æ ·å¼ */
    h1, h2, h3 {
        color: var(--text-color) !important;
        font-weight: 600 !important;
    }

    .footer-note {
        color: var(--text-color);
        opacity: 0.8;
        font-size: 13px;
        margin-top: 12px;
    }

    /* åŠ è½½å’Œè¿›åº¦æ ·å¼ */
    #loading, .progress-text {
        color: var(--text-color);
    }

    /* èŠå¤©è®°å½•æ ·å¼ */
    .chat-container {
        border: 1px solid var(--border-color);
        border-radius: 8px;
        margin-bottom: 16px;
        max-height: 80vh;
        height: 80vh !important;
        overflow-y: auto;
        background: var(--bg-color);
    }

    .chat-message {
        padding: 12px 16px;
        margin: 8px;
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.5;
    }

    .chat-message.user {
        background: var(--chat-user-bg);
        margin-left: 32px;
        border-top-right-radius: 4px;
    }

    .chat-message.assistant {
        background: var(--chat-assistant-bg);
        margin-right: 32px;
        border-top-left-radius: 4px;
    }

    .chat-message .timestamp {
        font-size: 12px;
        color: var(--text-color);
        opacity: 0.7;
        margin-bottom: 4px;
    }

    .chat-message .content {
        white-space: pre-wrap;
    }

    /* æŒ‰é’®ç»„æ ·å¼ */
    .button-row {
        display: flex;
        gap: 8px;
        margin-top: 8px;
    }

    .clear-button {
        background: var(--error-color) !important;
    }

    /* APIé…ç½®æç¤ºæ ·å¼ */
    .api-info {
        margin-top: 10px;
        padding: 10px;
        border-radius: 5px;
        background: var(--panel-bg);
        border: 1px solid var(--border-color);
    }

    /* æ–°å¢: æ•°æ®å¯è§†åŒ–å¡ç‰‡æ ·å¼ */
    .model-card {
        background: var(--panel-bg);
        border-radius: 8px;
        padding: 16px;
        border: 1px solid var(--border-color);
        margin-bottom: 16px;
    }

    .model-card h3 {
        margin-top: 0;
        border-bottom: 1px solid var(--border-color);
        padding-bottom: 8px;
    }

    .model-item {
        display: flex;
        margin-bottom: 8px;
    }

    .model-item .label {
        flex: 1;
        font-weight: 500;
    }

    .model-item .value {
        flex: 2;
    }

    /* æ•°æ®è¡¨æ ¼æ ·å¼ */
    .chunk-table {
        border-radius: 8px;
        overflow: hidden;
        border: 1px solid var(--border-color);
    }

    .chunk-table th, .chunk-table td {
        border: 1px solid var(--border-color);
        padding: 8px;
    }

    .chunk-detail-box {
        min-height: 200px;
        padding: 16px;
        background: var(--panel-bg);
        border-radius: 8px;
        border: 1px solid var(--border-color);
        font-family: monospace;
        white-space: pre-wrap;
        overflow-y: auto;
    }
    """
) as demo:
    gr.Markdown("# ğŸ§  æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    
    with gr.Tabs() as tabs:
        # ç¬¬ä¸€ä¸ªé€‰é¡¹å¡ï¼šé—®ç­”å¯¹è¯
        with gr.TabItem("ğŸ’¬ é—®ç­”å¯¹è¯"):
            with gr.Row(equal_height=True):
                # å·¦ä¾§æ“ä½œé¢æ¿ - è°ƒæ•´æ¯”ä¾‹ä¸ºåˆé€‚çš„å¤§å°
                with gr.Column(scale=5, elem_classes="left-panel"):
                    gr.Markdown("## ğŸ“‚ æ–‡æ¡£å¤„ç†åŒº")
                    with gr.Group():
                        file_input = gr.File(
                            label="ä¸Šä¼ PDFæ–‡æ¡£",
                            file_types=[".pdf"],
                            file_count="multiple"
                        )
                        upload_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                        upload_status = gr.Textbox(
                            label="å¤„ç†çŠ¶æ€",
                            interactive=False,
                            lines=2
                        )
                        file_list = gr.Textbox(
                            label="å·²å¤„ç†æ–‡ä»¶",
                            interactive=False,
                            lines=3,
                            elem_classes="file-list"
                        )
                    
                    # å°†é—®é¢˜è¾“å…¥åŒºç§»è‡³å·¦ä¾§é¢æ¿åº•éƒ¨
                    gr.Markdown("## â“ è¾“å…¥é—®é¢˜")
                    with gr.Group():
                        question_input = gr.Textbox(
                            label="è¾“å…¥é—®é¢˜",
                            lines=3,
                            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                            elem_id="question-input"
                        )
                        with gr.Row():
                            # æ·»åŠ è”ç½‘å¼€å…³
                            web_search_checkbox = gr.Checkbox(
                                label="å¯ç”¨è”ç½‘æœç´¢", 
                                value=False,
                                info="æ‰“å¼€åå°†åŒæ—¶æœç´¢ç½‘ç»œå†…å®¹ï¼ˆéœ€é…ç½®SERPAPI_KEYï¼‰"
                            )
                            
                            # æ·»åŠ æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
                            model_choice = gr.Dropdown(
                                choices=["ollama", "siliconflow"],
                                value="ollama",
                                label="æ¨¡å‹é€‰æ‹©",
                                info="é€‰æ‹©ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–äº‘ç«¯æ¨¡å‹"
                            )
                            
                        with gr.Row():
                            ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary", scale=2)
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary", elem_classes="clear-button", scale=1)
                    
                    # æ·»åŠ APIé…ç½®æç¤ºä¿¡æ¯
                    api_info = gr.HTML(
                        """
                        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
                            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
                            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
                            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
                        </div>
                        """
                    )

                # å³ä¾§å¯¹è¯åŒº - è°ƒæ•´æ¯”ä¾‹
                with gr.Column(scale=7, elem_classes="right-panel"):
                    gr.Markdown("## ğŸ“ å¯¹è¯è®°å½•")
                    
                    # å¯¹è¯è®°å½•æ˜¾ç¤ºåŒº
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯å†å²",
                        height=600,  # å¢åŠ é«˜åº¦
                        elem_classes="chat-container",
                        show_label=False
                    )
                    
                    status_display = gr.HTML("", elem_id="status-display")
                    gr.Markdown("""
                    <div class="footer-note">
                        *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                        *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
                    </div>
                    """)
        
        # ç¬¬äºŒä¸ªé€‰é¡¹å¡ï¼šåˆ†å—å¯è§†åŒ–
        with gr.TabItem("ğŸ“Š åˆ†å—å¯è§†åŒ–"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("## ğŸ’¡ ç³»ç»Ÿæ¨¡å‹ä¿¡æ¯")
                    
                    # æ˜¾ç¤ºç³»ç»Ÿæ¨¡å‹ä¿¡æ¯å¡ç‰‡
                    models_info = get_system_models_info()
                    with gr.Group(elem_classes="model-card"):
                        gr.Markdown("### æ ¸å¿ƒæ¨¡å‹ä¸æŠ€æœ¯")
                        
                        for key, value in models_info.items():
                            with gr.Row():
                                gr.Markdown(f"**{key}**:", elem_classes="label")
                                gr.Markdown(f"{value}", elem_classes="value")
                
                with gr.Column(scale=2):
                    gr.Markdown("## ğŸ“„ æ–‡æ¡£åˆ†å—ç»Ÿè®¡")
                    refresh_chunks_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ†å—æ•°æ®", variant="primary")
                    chunks_status = gr.Markdown("ç‚¹å‡»æŒ‰é’®æŸ¥çœ‹åˆ†å—ç»Ÿè®¡")
            
            # åˆ†å—æ•°æ®è¡¨æ ¼å’Œè¯¦æƒ…
            with gr.Row():
                chunks_data = gr.Dataframe(
                    headers=["æ¥æº", "åºå·", "å­—ç¬¦æ•°", "åˆ†è¯æ•°", "å†…å®¹é¢„è§ˆ"],
                    elem_classes="chunk-table",
                    interactive=False,
                    wrap=True,
                    row_count=(10, "dynamic")
                )
            
            with gr.Row():
                chunk_detail_text = gr.Textbox(
                    label="åˆ†å—è¯¦æƒ…",
                    placeholder="ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡ŒæŸ¥çœ‹å®Œæ•´å†…å®¹...",
                    lines=8,
                    elem_classes="chunk-detail-box"
                )
                
            gr.Markdown("""
            <div class="footer-note">
                * ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡Œå¯æŸ¥çœ‹è¯¥åˆ†å—çš„å®Œæ•´å†…å®¹<br>
                * åˆ†è¯æ•°è¡¨ç¤ºä½¿ç”¨jiebaåˆ†è¯åçš„tokenæ•°é‡
            </div>
            """)

    # è°ƒæ•´åçš„åŠ è½½æç¤º
    gr.HTML("""
    <div id="loading" style="text-align:center;padding:20px;">
        <h3>ğŸ”„ ç³»ç»Ÿåˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™...</h3>
    </div>
    """)

    # è¿›åº¦æ˜¾ç¤ºç»„ä»¶è°ƒæ•´åˆ°å·¦ä¾§é¢æ¿ä¸‹æ–¹
    with gr.Row(visible=False) as progress_row:
        gr.HTML("""
        <div class="progress-text">
            <span>å½“å‰è¿›åº¦ï¼š</span>
            <span id="current-step" style="color: #2b6de3;">åˆå§‹åŒ–...</span>
            <span id="progress-percent" style="margin-left:15px;color: #e32b2b;">0%</span>
        </div>
        """)

    # å®šä¹‰å‡½æ•°å¤„ç†äº‹ä»¶
    def clear_chat_history():
        return None, "å¯¹è¯å·²æ¸…ç©º"

    def process_chat(question, history, enable_web_search, model_choice):
        if history is None:
            history = []
        
        # æ›´æ–°æ¨¡å‹é€‰æ‹©ä¿¡æ¯çš„æ˜¾ç¤º
        api_text = """
        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
        </div>
        """ % (
            "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨", 
            "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
            "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
        )
        
        # å¦‚æœé—®é¢˜ä¸ºç©ºï¼Œä¸å¤„ç†
        if not question or question.strip() == "":
            history.append(("", "é—®é¢˜ä¸èƒ½ä¸ºç©ºï¼Œè¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ã€‚"))
            return history, "", api_text
        
        # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²
        history.append((question, ""))
        
        # åˆ›å»ºç”Ÿæˆå™¨
        resp_generator = stream_answer(question, enable_web_search, model_choice)
        
        # æµå¼æ›´æ–°å›ç­”
        for response, status in resp_generator:
            history[-1] = (question, response)
            yield history, "", api_text

    def update_api_info(enable_web_search, model_choice):
        api_text = """
        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
        </div>
        """ % (
            "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨", 
            "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
            "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
        )
        return api_text

    # ç»‘å®šUIäº‹ä»¶
    upload_btn.click(
        process_multiple_pdfs,
        inputs=[file_input],
        outputs=[upload_status, file_list],
        show_progress=True
    )

    # ç»‘å®šæé—®æŒ‰é’®
    ask_btn.click(
        process_chat,
        inputs=[question_input, chatbot, web_search_checkbox, model_choice],
        outputs=[chatbot, question_input, api_info]
    )

    # ç»‘å®šæ¸…ç©ºæŒ‰é’®
    clear_btn.click(
        clear_chat_history,
        inputs=[],
        outputs=[chatbot, status_display]
    )

    # å½“åˆ‡æ¢è”ç½‘æœç´¢æˆ–æ¨¡å‹é€‰æ‹©æ—¶æ›´æ–°APIä¿¡æ¯
    web_search_checkbox.change(
        update_api_info,
        inputs=[web_search_checkbox, model_choice],
        outputs=[api_info]
    )
    
    model_choice.change(
        update_api_info,
        inputs=[web_search_checkbox, model_choice],
        outputs=[api_info]
    )
    
    # æ–°å¢ï¼šåˆ†å—å¯è§†åŒ–åˆ·æ–°æŒ‰é’®äº‹ä»¶
    refresh_chunks_btn.click(
        fn=get_document_chunks,
        outputs=[chunks_data, chunks_status]
    )
    
    # æ–°å¢ï¼šåˆ†å—è¡¨æ ¼ç‚¹å‡»äº‹ä»¶
    chunks_data.select(
        fn=show_chunk_details,
        inputs=chunks_data,
        outputs=chunk_detail_text
    )

# ä¿®æ”¹JavaScriptæ³¨å…¥éƒ¨åˆ†
demo._js = """
function gradioApp() {
    // è®¾ç½®é»˜è®¤ä¸»é¢˜ä¸ºæš—è‰²
    document.documentElement.setAttribute('data-theme', 'dark');
    
    const observer = new MutationObserver((mutations) => {
        document.getElementById("loading").style.display = "none";
        const progress = document.querySelector('.progress-text');
        if (progress) {
            const percent = document.querySelector('.progress > div')?.innerText || '';
            const step = document.querySelector('.progress-description')?.innerText || '';
            document.getElementById('current-step').innerText = step;
            document.getElementById('progress-percent').innerText = percent;
        }
    });
    observer.observe(document.body, {childList: true, subtree: true});
}

function toggleTheme() {
    const root = document.documentElement;
    const currentTheme = root.getAttribute('data-theme');
    const newTheme = currentTheme === 'light' ? 'dark' : 'light';
    root.setAttribute('data-theme', newTheme);
}

// åˆå§‹åŒ–ä¸»é¢˜
document.addEventListener('DOMContentLoaded', () => {
    document.documentElement.setAttribute('data-theme', 'dark');
});
"""

# ä¿®æ”¹ç«¯å£æ£€æŸ¥å‡½æ•°
def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0  # æ›´å¯é çš„æ£€æµ‹æ–¹å¼

def check_environment():
    """ç¯å¢ƒä¾èµ–æ£€æŸ¥"""
    try:
        # æ·»åŠ æ¨¡å‹å­˜åœ¨æ€§æ£€æŸ¥
        model_check = session.post(
            "http://localhost:11434/api/show",
            json={"name": "deepseek-r1:7b"},
            timeout=10
        )
        if model_check.status_code != 200:
            print("æ¨¡å‹æœªåŠ è½½ï¼è¯·å…ˆæ‰§è¡Œï¼š")
            print("ollama pull deepseek-r1:7b")
            return False
            
        # åŸæœ‰æ£€æŸ¥ä¿æŒä¸å˜...
        response = session.get(
            "http://localhost:11434/api/tags",
            proxies={"http": None, "https": None},  # ç¦ç”¨ä»£ç†
            timeout=5
        )
        if response.status_code != 200:
            print("OllamaæœåŠ¡å¼‚å¸¸ï¼Œè¿”å›çŠ¶æ€ç :", response.status_code)
            return False
        return True
    except Exception as e:
        print("Ollamaè¿æ¥å¤±è´¥:", str(e))
        return False

# æ–¹æ¡ˆ2ï¼šç¦ç”¨æµè§ˆå™¨ç¼“å­˜ï¼ˆæ·»åŠ metaæ ‡ç­¾ï¼‰
gr.HTML("""
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Expires" content="0">
""")

# æ¢å¤ä¸»ç¨‹åºå¯åŠ¨éƒ¨åˆ†
if __name__ == "__main__":
    if not check_environment():
        exit(1)
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)
    
    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)
        
    try:
        ollama_check = session.get("http://localhost:11434", timeout=5)
        if ollama_check.status_code != 200:
            print("OllamaæœåŠ¡æœªæ­£å¸¸å¯åŠ¨ï¼")
            print("è¯·å…ˆæ‰§è¡Œï¼šollama serve å¯åŠ¨æœåŠ¡")
            exit(1)
            
        webbrowser.open(f"http://127.0.0.1:{selected_port}")
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False,
            height=900
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}")

