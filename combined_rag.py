import os
import socket
import json
import webbrowser
import logging
from io import StringIO
import time
import re
import markdown
from typing import List, Dict
import hashlib

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from pdfminer.high_level import extract_text_to_fp
from langchain.text_splitter import RecursiveCharacterTextSplitter
import gradio as gr
import chromadb
from chromadb.config import Settings
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
SERPAPI_KEY = os.getenv("SERPAPI_KEY")  # è¯·åœ¨.envä¸­è®¾ç½® SERPAPI_KEY
SEARCH_ENGINE = "google"  # å¯æ ¹æ®éœ€è¦æ”¹ä¸ºå…¶ä»–æœç´¢å¼•æ“

# åˆå§‹åŒ–æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
# ä¸ºäº†å®ç°å¤šæºå‘é‡ç»Ÿä¸€ï¼Œè¯·ç¡®ä¿ PDF å’Œç½‘ç»œç»“æœä½¿ç”¨ç›¸åŒçš„åµŒå…¥ç©ºé—´
# å¦‚æœæœ‰æœ¬åœ°éƒ¨ç½²çš„ "text-embedding-3-small" æ¨¡å‹ï¼Œä¹Ÿå¯æ›¿æ¢ä½¿ç”¨ï¼Œä½†éœ€ä¿è¯ä¸ PDF å‘é‡ä¸€è‡´
EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')

# åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯ä»¥åŠå…±äº«é›†åˆ
CHROMA_CLIENT = chromadb.PersistentClient(
    path="./chroma_db",
    settings=Settings(anonymized_telemetry=False)
)
COLLECTION = CHROMA_CLIENT.get_or_create_collection("rag_docs")

# è®¾ç½®é‡è¯• session
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
session.mount('http://', HTTPAdapter(max_retries=retries))

#########################################
# SerpAPI ç½‘ç»œæŸ¥è¯¢åŠå‘é‡åŒ–å¤„ç†å‡½æ•°
#########################################
def serpapi_search(query: str, num_results: int = 5) -> list[dict]:
    """
    æ‰§è¡Œ SerpAPI æœç´¢ï¼Œå¹¶è¿”å›è§£æåçš„ç»“æ„åŒ–ç»“æœ
    """
    if not SERPAPI_KEY:
        raise ValueError("æœªè®¾ç½® SERPAPI_KEY ç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ API å¯†é’¥ã€‚")
    try:
        params = {
            "engine": SEARCH_ENGINE,
            "q": query,
            "api_key": SERPAPI_KEY,
            "num": num_results,
            "hl": "zh-CN",  # ä¸­æ–‡ç•Œé¢
            "gl": "cn"
        }
        response = requests.get("https://serpapi.com/search", params=params, timeout=15)
        response.raise_for_status()
        search_data = response.json()
        return _parse_serpapi_results(search_data)
    except Exception as e:
        logging.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
        return []

def _parse_serpapi_results(data: dict) -> List[Dict[str, str]]:
    """è§£æ SerpAPI è¿”å›çš„åŸå§‹æ•°æ®"""
    results = []
    if "organic_results" in data:
        for item in data["organic_results"]:
            result = {
                "title": item.get("title"),
                "url": item.get("link"),
                "snippet": item.get("snippet"),
                "timestamp": item.get("date")  # è‹¥æœ‰æ—¶é—´ä¿¡æ¯ï¼Œå¯é€‰
            }
            results.append(result)
    # å¦‚æœæœ‰çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥æ·»åŠ ç½®é¡¶ï¼ˆå¯é€‰ï¼‰
    if "knowledge_graph" in data:
        kg = data["knowledge_graph"]
        results.insert(0, {
            "title": kg.get("title"),
            "url": kg.get("source", {}).get("link", ""),
            "snippet": kg.get("description"),
            "source": "knowledge_graph"
        })
    return results

def update_web_results(query: str, num_results: int = 5) -> list[dict]:
    """
    åŸºäº SerpAPI æœç´¢ç»“æœï¼Œå‘é‡åŒ–å¹¶å­˜å‚¨åˆ° ChromaDB
    ä¸ºç½‘ç»œç»“æœæ·»åŠ å…ƒæ•°æ®ï¼ŒID æ ¼å¼ä¸º "web_{index}"
    """
    results = serpapi_search(query, num_results)
    if not results:
        return []
    # åˆ é™¤æ—§çš„ç½‘ç»œæœç´¢ç»“æœ
    existing_ids = COLLECTION.get()['ids']
    web_ids = [doc_id for doc_id in existing_ids if doc_id.startswith("web_")]
    if web_ids:
        COLLECTION.delete(ids=web_ids)
    docs = []
    metadatas = []
    ids = []
    for idx, res in enumerate(results):
        text = f"æ ‡é¢˜ï¼š{res.get('title', '')}\næ‘˜è¦ï¼š{res.get('snippet', '')}"
        docs.append(text)
        meta = {"source": "web", "url": res.get("url", ""), "title": res.get("title")}
        meta["content_hash"] = hashlib.md5(text.encode()).hexdigest()[:8]
        metadatas.append(meta)
        ids.append(f"web_{idx}")
    embeddings = EMBED_MODEL.encode(docs)
    metadatas = [{"source": "pdf"} if not meta else meta for meta in metadatas]
    COLLECTION.add(ids=ids, embeddings=embeddings.tolist(), documents=docs, metadatas=metadatas)
    return results

#########################################
# PDF æ–‡æ¡£å¤„ç†ï¼ˆæœ¬åœ°çŸ¥è¯†åº“æ›´æ–°ï¼‰
#########################################
def extract_text(filepath: str) -> str:
    """ä½¿ç”¨ PDFMiner æå– PDF æ–‡æœ¬"""
    output = StringIO()
    with open(filepath, 'rb') as file:
        extract_text_to_fp(file, output)
    return output.getvalue()

def process_pdf(file, progress=gr.Progress()):
    """
    å¤„ç† PDF æ–‡æ¡£ï¼šæå–æ–‡æœ¬ã€åˆ†å‰²æ–‡æœ¬ã€ç”ŸæˆåµŒå…¥ã€å­˜å‚¨åˆ° ChromaDB ä¸­
    æ¯ä¸ªæ–‡æœ¬å—æ·»åŠ å…ƒæ•°æ® source: pdf
    """
    try:
        progress(0.2, desc="è§£æPDF...")
        text = extract_text(file.name)
        
        progress(0.4, desc="åˆ†å‰²æ–‡æœ¬...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=50
        )
        chunks = text_splitter.split_text(text)
        
        progress(0.6, desc="ç”ŸæˆåµŒå…¥...")
        embeddings = EMBED_MODEL.encode(chunks)
        
        progress(0.8, desc="å­˜å‚¨å‘é‡...")
        # åˆ é™¤æ—§çš„ PDF æ–‡æ¡£æ•°æ®ï¼ˆæ ¹æ® id å‰ç¼€ pdf_ï¼‰
        existing_ids = COLLECTION.get()['ids']
        pdf_ids = [doc_id for doc_id in existing_ids if doc_id.startswith("pdf_")]
        if pdf_ids:
            COLLECTION.delete(ids=pdf_ids)
        ids = [f"pdf_{i}" for i in range(len(chunks))]
        metadatas = [{"source": "pdf"} for _ in chunks]
        metadatas = [{"source": "pdf", "content_hash": hashlib.md5(chunk.encode()).hexdigest()[:8]} for chunk in chunks]
        metadatas = [{"source": "pdf"} if not meta else meta for meta in metadatas]
        COLLECTION.add(ids=ids, embeddings=embeddings.tolist(), documents=chunks, metadatas=metadatas)
        
        progress(1.0, desc="å®Œæˆ!")
        return "PDFå¤„ç†å®Œæˆï¼Œå·²å­˜å‚¨ {} ä¸ªæ–‡æœ¬å—".format(len(chunks))
    except Exception as e:
        return f"å¤„ç†å¤±è´¥: {str(e)}"

#########################################
# æ–°å¢çŸ›ç›¾æ£€æµ‹å‡½æ•°ï¼ˆéœ€æ”¾åœ¨è°ƒç”¨å‰ï¼‰
#########################################
def detect_conflicts(sources):
    """ç²¾å‡†çŸ›ç›¾æ£€æµ‹ç®—æ³•"""
    key_facts = {}
    for item in sources:
        # ä»itemå­—å…¸ä¸­è·å–excerpt
        facts = extract_facts(item['excerpt'])
        for fact, value in facts.items():
            if fact in key_facts:
                if key_facts[fact] != value:
                    return True
            else:
                key_facts[fact] = value
    return False

def extract_facts(text):
    """ä»æ–‡æœ¬æå–å…³é”®äº‹å®ï¼ˆç¤ºä¾‹é€»è¾‘ï¼‰"""
    facts = {}
    # æå–æ•°å€¼å‹äº‹å®
    numbers = re.findall(r'\b\d{4}å¹´|\b\d+%', text)
    if numbers:
        facts['å…³é”®æ•°å€¼'] = numbers
    # æå–æŠ€æœ¯æœ¯è¯­
    if "äº§ä¸šå›¾è°±" in text:
        facts['æŠ€æœ¯æ–¹æ³•'] = list(set(re.findall(r'[A-Za-z]+æ¨¡å‹|[A-Z]{2,}ç®—æ³•', text)))
    return facts

#########################################
# å¤šæºç»“æœæ•´åˆåŠé—®ç­”ç”Ÿæˆå‡½æ•°
#########################################
def combined_query_answer(question, progress=gr.Progress()):
    """
    åŸºäºæœ¬åœ° PDF ä¸ç½‘ç»œæœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ï¼ˆå¸¦è¯¦ç»†è¿›åº¦ï¼‰
    """
    try:
        # å¢å¼ºç‰ˆæ—¶é—´æ•æ„Ÿæ€§æ£€æµ‹
        time_keywords = {
            "æ—¶é—´ç›¸å…³": ["æœ€æ–°", "ä»Šå¹´", "å½“å‰", "æœ€è¿‘", "åˆšåˆš", "æ—¥å‰", "è¿‘æ—¥", "è¿‘æœŸ"],
            "å¹´ä»½æ¨¡å¼": r"\b(20\d{2}|ä»Šå¹´|æ˜å¹´|å»å¹´)\b",
            "æ—¶é—´å‰¯è¯": ["æœ€è¿‘", "ç›®å‰", "ç°é˜¶æ®µ", "å½“ä¸‹", "æ­¤åˆ»"]
        }
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¢å¼ºæ£€æµ‹
        time_sensitive = (
            any(word in question for word in time_keywords["æ—¶é—´ç›¸å…³"]) or
            re.search(time_keywords["å¹´ä»½æ¨¡å¼"], question) or
            any(adv in question for adv in time_keywords["æ—¶é—´å‰¯è¯"])
        )
        
        # é˜¶æ®µ1ï¼šåˆå§‹åŒ–å¤„ç†
        progress(0.05, desc="ğŸ”„ æ­£åœ¨åˆ†æé—®é¢˜ç±»å‹...")
        
        # é˜¶æ®µ2ï¼šç½‘ç»œæœç´¢å¤„ç†
        if time_sensitive:
            progress(0.1, desc="ğŸŒ æ­£åœ¨è·å–æœ€æ–°ç½‘ç»œç»“æœ (0/3)")
            update_steps = [
                "æ‰§è¡Œæœç´¢è¯·æ±‚",
                "è§£ææœç´¢ç»“æœ",
                "å‘é‡åŒ–å­˜å‚¨"
            ]
            for i, step in enumerate(update_steps):
                progress(0.1 + i*0.1, desc=f"ğŸŒ {step} ({i+1}/3)")
            results = serpapi_search(question)  # çœŸå®è€—æ—¶æ“ä½œ
            
            progress(0.3, desc="ğŸŒ è§£ææœç´¢ç»“æœ")
            parsed = _parse_serpapi_results(results)
            
            progress(0.5, desc="ğŸŒ å‘é‡åŒ–å­˜å‚¨")
            update_web_results(question)  # ä½¿ç”¨å·²å®šä¹‰çš„æ›´æ–°å‡½æ•°
        else:
            progress(0.4, desc="â© è·³è¿‡ç½‘ç»œæœç´¢")

        # é˜¶æ®µ3ï¼šå‘é‡æ£€ç´¢
        progress_steps = [
            (0.4, "ç”Ÿæˆé—®é¢˜åµŒå…¥"),
            (0.5, "æ£€ç´¢æœ¬åœ°çŸ¥è¯†åº“"),
            (0.6, "æ’åºå¤šæºç»“æœ")
        ]
        for percent, desc in progress_steps:
            progress(percent, desc=f"ğŸ” {desc}")

        # é˜¶æ®µ4ï¼šç”Ÿæˆå›ç­”
        progress(0.7, desc="ğŸ’¡ æ­£åœ¨æ„å»ºæç¤ºè¯")
        question_embedding = EMBED_MODEL.encode([question]).tolist()
        
        progress(0.8, desc="ğŸ¤– è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”")
        query_results = COLLECTION.query(
            query_embeddings=question_embedding,
            n_results=10,
            include=["documents", "metadatas"]
        )
        
        # ä¿®å¤æ–‡æ¡£å’Œå…ƒæ•°æ®çš„å¯¹é½é—®é¢˜
        combined_items = []
        documents = query_results.get("documents", [[]])[0]  # é˜²æ­¢ç©ºå€¼
        metadatas = query_results.get("metadatas", [[]])[0]  # é˜²æ­¢ç©ºå€¼
        
        # ç¡®ä¿æ–‡æ¡£å’Œå…ƒæ•°æ®æ•°é‡ä¸€è‡´
        max_length = max(len(documents), len(metadatas))
        for idx in range(max_length):
            doc = documents[idx] if idx < len(documents) else ""
            meta = metadatas[idx] if idx < len(metadatas) else {}
            
            # ç¡®ä¿å…ƒæ•°æ®æ˜¯å­—å…¸ç±»å‹
            safe_meta = meta if isinstance(meta, dict) else {}
            source_type = safe_meta.get("source", "unknown")
            
            combined_items.append({
                "type": source_type,
                "url": safe_meta.get("url", ""),
                "excerpt": (doc[:200] + "...") if doc else "",
                "title": safe_meta.get("title", "æ— æ ‡é¢˜")  # æ–°å¢æ ‡é¢˜å­—æ®µ
            })

        # ä¿®æ”¹æ’åºé€»è¾‘ï¼ˆç¡®ä¿ç½‘ç»œç»“æœä¼˜å…ˆï¼‰
        if time_sensitive:
            sorted_items = sorted(
                combined_items,
                key=lambda x: (x["type"] != "web", -len(x["excerpt"]))
            )
        else:
            sorted_items = sorted(
                combined_items,
                key=lambda x: (-len(x["excerpt"]), x["type"])
            )
        
        # ä¿®æ”¹åçš„ä¸Šä¸‹æ–‡æ„å»ºéƒ¨åˆ†
        context_parts = []
        for idx, item in enumerate(sorted_items, 1):
            if item["type"] == "web":
                context_parts.append(f"[ç½‘ç»œç»“æœ {idx}] {item['excerpt']} (é“¾æ¥: {item['url']})")
            else:
                context_parts.append(f"[æœ¬åœ°æ–‡æ¡£ {idx}] {item['excerpt']}")
        context = "\n\n".join(context_parts)
        
        # æ„å»ºæç¤ºè¯æ¨¡æ¿ï¼Œå¹¶æé†’æ¨¡å‹æ³¨æ„çŸ›ç›¾æ£€æµ‹
        prompt = (
            f"è¯·æ ¹æ®ä»¥ä¸‹æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œæœç´¢ç»“æœå›ç­”é—®é¢˜ï¼š\n{context}\n\n"
            "æ³¨æ„ï¼šè‹¥æœ¬åœ°ä¸ç½‘ç»œç»“æœå­˜åœ¨çŸ›ç›¾ï¼Œè¯·åˆ†åˆ«æ ‡æ˜å¹¶è¯´æ˜æ•°æ®æ¥æºã€‚\n\n"
            f"é—®é¢˜ï¼š{question}"
        )
        
        # æ–°å¢çŸ›ç›¾æ£€æµ‹æ¨¡å—
        conflict_detected = detect_conflicts(sorted_items)
        
        if conflict_detected:
            credible_sources = [s for s in sorted_items if evaluate_source_credibility(s) > 0.7]
            if credible_sources:
                prompt += "\næ³¨æ„ï¼šä»¥ä¸‹é«˜å¯ä¿¡æ¥æºå»ºè®®ä¼˜å…ˆå‚è€ƒï¼š\n"
                prompt += "\n".join(f"- {s['url']}" for s in credible_sources)
        
        progress(0.95, desc="âœ… æ­£åœ¨æ ¼å¼åŒ–æœ€ç»ˆç­”æ¡ˆ")
        time.sleep(0.5)
        
        progress(1.0, desc="ğŸ‰ å¤„ç†å®Œæˆï¼")
        result = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "deepseek-r1:1.5b",
                "prompt": prompt,
                "stream": False
            },
            timeout=120,
            headers={'Connection': 'close'}
        ).json().get("response", "æœªè·å–åˆ°æœ‰æ•ˆå›ç­”")
        return format_answer(result, sorted_items)
        
    except Exception as e:
        progress(1.0, desc="âŒ é‡åˆ°é”™è¯¯")
        return f"ç³»ç»Ÿé”™è¯¯: {str(e)}"

def format_answer(response_text, sources):
    """ç”Ÿæˆè‡ªé€‚åº”ä¸»é¢˜çš„HTMLå›ç­”"""
    return f"""
    <style>
        :root {{
            --background: #f8f9fa;
            --text: #333;
            --code-bg: #f4f4f4;
            --border: #e0e0e0;
        }}
        
        @media (prefers-color-scheme: dark) {{
            :root {{
                --background: #2d2d2d;
                --text: #e0e0e0;
                --code-bg: #1e1e1e;
                --border: #404040;
            }}
        }}

        .answer-container {{
            padding: 20px;
            background: var(--background);
            color: var(--text);
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            transition: all 0.3s ease;
        }}
        
        .source-item {{
            margin: 10px 0;
            padding: 10px;
            border-left: 3px solid var(--border);
            background: rgba(255, 255, 255, 0.05);
        }}
        
        pre {{
            background: var(--code-bg);
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
            border: 1px solid var(--border);
        }}
        
        a {{ color: #58a6ff; }}
    </style>
    <div class="answer-container">
        <div class="answer-content">{markdown.markdown(response_text)}</div>
        <div class="sources-section">
            <h3>ğŸ“– å‚è€ƒæ¥æº</h3>
            {_format_sources(sources)}
        </div>
    </div>
    """

def _format_sources(sources):
    """æ ¼å¼åŒ–æ¥æºä¿¡æ¯"""
    items = []
    for idx, source in enumerate(sources, 1):
        badge_color = "#4CAF50" if source['type'] == 'web' else "#2196F3"
        items.append(f"""
        <div class="source-item">
            <div class="source-header">
                <span class="source-badge" style="background:{badge_color}">
                    {source['type']}
                </span>
                <a href="{source['url']}" target="_blank">{source.get('title', 'æ¥æº'+str(idx))}</a>
            </div>
            <div class="excerpt">{source['excerpt']}</div>
        </div>
        """)
    return "\n".join(items)

#########################################
# ç¯å¢ƒä¸ç«¯å£æ£€æµ‹å‡½æ•°
#########################################
def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0

def check_environment():
    """
    ç¯å¢ƒæ£€æŸ¥ï¼š
    1. æ£€æŸ¥å¤§æ¨¡å‹æœåŠ¡æ˜¯å¦åŠ è½½ï¼ˆä»¥ deepseek-r1:7b ä¸ºä¾‹ï¼‰ï¼›
    2. æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€ã€‚
    """
    try:
        model_check = session.post(
            "http://localhost:11434/api/show",
            json={"name": "deepseek-r1:7b"},
            timeout=10
        )
        if model_check.status_code != 200:
            print("æ¨¡å‹æœªåŠ è½½ï¼è¯·å…ˆæ‰§è¡Œï¼š")
            print("ollama pull deepseek-r1:7b")
            return False
        response = session.get(
            "http://localhost:11434/api/tags",
            proxies={"http": None, "https": None},
            timeout=5
        )
        if response.status_code != 200:
            print("OllamaæœåŠ¡å¼‚å¸¸ï¼Œè¿”å›çŠ¶æ€ç :", response.status_code)
            return False
        return True
    except Exception as e:
        print("Ollamaè¿æ¥å¤±è´¥:", str(e))
        return False

#########################################
# Gradio ç•Œé¢æ„å»º
#########################################
with gr.Blocks(
    css="""
    .gradio-container { max-height: 90vh !important; overflow-y: auto; }
    .left-panel, .right-panel { height: auto !important; }
    .answer-box { max-height: 60vh; overflow-y: auto; }
    .progress-tracker-container { position: static; }
    """
) as demo:
    gr.Markdown("# ğŸ§  å¤šæºæ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    
    with gr.Row():
        # å·¦ä¾§é¢æ¿é«˜åº¦é™åˆ¶
        with gr.Column(scale=1, elem_classes="left-panel"):
            gr.Markdown("## ğŸ“‚ æ–‡æ¡£å¤„ç†åŒº")
            with gr.Group():
                file_input = gr.File(label="ä¸Šä¼ PDFæ–‡æ¡£", file_types=[".pdf"])
                upload_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                upload_status = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False)
            
            gr.Markdown("## â“ æé—®åŒº")
            with gr.Group():
                question_input = gr.Textbox(
                    label="è¾“å…¥é—®é¢˜",
                    lines=4,
                    placeholder="ä¾‹å¦‚ï¼šæœ¬æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                    elem_id="question-input"
                )
                ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary")
                status_display = gr.HTML("", elem_id="status-display")
        
        # å³ä¾§é¢æ¿é«˜åº¦é™åˆ¶
        with gr.Column(scale=3, elem_classes="right-panel"):
            gr.Markdown("## ğŸ“ ç­”æ¡ˆå±•ç¤º")
            progress_steps = gr.HTML()
            real_time_status = gr.HTML()  # å°†çŠ¶æ€ç»„ä»¶ç§»å…¥å³ä¾§é¢æ¿
            answer_output = gr.HTML(elem_classes="answer-box")
    
    # åŠ è½½æç¤º
    # gr.HTML("""
    # <div id="loading" style="text-align:center;padding:20px;">
    #     <h3>ğŸ”„ ç³»ç»Ÿåˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™...</h3>
    # </div>
    # """)
    
    # æŒ‰é’®äº‹ä»¶ç»‘å®š
    upload_btn.click(
        fn=process_pdf,
        inputs=file_input,
        outputs=upload_status
    )
    ask_btn.click(
        fn=combined_query_answer,
        inputs=question_input,
        outputs=answer_output,
        show_progress="full",  # æ”¹ä¸ºå®Œæ•´è¿›åº¦æ¡
        api_name="ask_question"
    )

#########################################
# ä¸»ç¨‹åºå¯åŠ¨
#########################################
if __name__ == "__main__":
    if not check_environment():
        exit(1)
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)
    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)
    try:
        ollama_check = session.get("http://localhost:11434", timeout=5)
        if ollama_check.status_code != 200:
            print("OllamaæœåŠ¡æœªæ­£å¸¸å¯åŠ¨ï¼")
            print("è¯·å…ˆæ‰§è¡Œï¼šollama serve å¯åŠ¨æœåŠ¡")
            exit(1)
        webbrowser.open(f"http://127.0.0.1:{selected_port}")
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}") 

def generate_answer_with_context(question, context):
    """æ”¹è¿›åçš„å›ç­”ç”Ÿæˆé€»è¾‘"""
    # æ–°å¢çŸ›ç›¾æ£€æµ‹æ¨¡å—
    conflict_detected = detect_conflicts(context['sources'])
    
    prompt = f"""
    æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š
    {json.dumps(context['documents'], ensure_ascii=False, indent=2)}
    é—®é¢˜ï¼š{question}
    
    å›ç­”è¦æ±‚ï¼š
    1. åŒºåˆ†ä¿¡æ¯æ¥æºäºã€æœ¬åœ°æ–‡æ¡£ã€‘æˆ–ã€ç½‘ç»œç»“æœã€‘
    2. ä»…å½“æ˜ç¡®çŸ›ç›¾æ—¶è¯´æ˜å·®å¼‚ï¼ˆå½“å‰æ£€æµ‹çŠ¶æ€ï¼š{'å‘ç°çŸ›ç›¾éœ€è¯´æ˜' if conflict_detected else 'æœªå‘ç°æ˜ç¡®çŸ›ç›¾'}ï¼‰
    3. æ—¶é—´æ•æ„Ÿé—®é¢˜ä¼˜å…ˆä½¿ç”¨æœ€æ–°ç½‘ç»œç»“æœ
    """
    # ...åç»­ç”Ÿæˆé€»è¾‘ä¸å˜...

def evaluate_source_credibility(source):
    """è¯„ä¼°æ¥æºå¯ä¿¡åº¦"""
    credibility_scores = {
        "gov.cn": 0.9,
        "weixin": 0.7,
        "zhihu": 0.6
    }
    domain = re.search(r'//([^/]+)', source['url']).group(1)
    return credibility_scores.get(domain, 0.5) 