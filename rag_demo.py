import gradio as gr
from pdfminer.high_level import extract_text_to_fp
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import requests
import json
from io import StringIO
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import socket
import webbrowser
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time
from datetime import datetime

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ è¶…æ—¶è®¾ç½®
import requests
requests.adapters.DEFAULT_RETRIES = 3  # å¢åŠ é‡è¯•æ¬¡æ•°

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ç¯å¢ƒå˜é‡è®¾ç½®
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # ç¦ç”¨oneDNNä¼˜åŒ–

# åœ¨æ–‡ä»¶æœ€å¼€å¤´æ·»åŠ ä»£ç†é…ç½®
import os
os.environ['NO_PROXY'] = 'localhost,127.0.0.1'  # æ–°å¢ä»£ç†ç»•è¿‡è®¾ç½®

# åˆå§‹åŒ–ç»„ä»¶
EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
CHROMA_CLIENT = chromadb.PersistentClient(
    path="./chroma_db",
    settings=chromadb.Settings(anonymized_telemetry=False)
)
COLLECTION = CHROMA_CLIENT.get_or_create_collection("rag_docs")

logging.basicConfig(level=logging.INFO)

print("Gradio version:", gr.__version__)  # æ·»åŠ ç‰ˆæœ¬è¾“å‡º

# åœ¨åˆå§‹åŒ–ç»„ä»¶åæ·»åŠ ï¼š
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
session.mount('http://', HTTPAdapter(max_retries=retries))

# æ·»åŠ æ–‡ä»¶å¤„ç†çŠ¶æ€è·Ÿè¸ª
class FileProcessor:
    def __init__(self):
        self.processed_files = {}  # å­˜å‚¨å·²å¤„ç†æ–‡ä»¶çš„çŠ¶æ€
        
    def clear_files(self):
        """æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶è®°å½•"""
        self.processed_files = {}
        
    def add_file(self, file_name):
        self.processed_files[file_name] = {
            'status': 'ç­‰å¾…å¤„ç†',
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'chunks': 0
        }
        
    def update_status(self, file_name, status, chunks=None):
        if file_name in self.processed_files:
            self.processed_files[file_name]['status'] = status
            if chunks is not None:
                self.processed_files[file_name]['chunks'] = chunks
                
    def get_file_list(self):
        return [
            f"ğŸ“„ {fname} | {info['status']}"
            for fname, info in self.processed_files.items()
        ]

file_processor = FileProcessor()

def extract_text(filepath):
    """æ”¹è¿›çš„PDFæ–‡æœ¬æå–æ–¹æ³•"""
    output = StringIO()
    with open(filepath, 'rb') as file:
        extract_text_to_fp(file, output)
    return output.getvalue()

def process_multiple_pdfs(files, progress=gr.Progress()):
    """å¤„ç†å¤šä¸ªPDFæ–‡ä»¶"""
    if not files:
        return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„PDFæ–‡ä»¶", []
    
    try:
        # æ¸…ç©ºå‘é‡æ•°æ®åº“
        progress(0.1, desc="æ¸…ç†å†å²æ•°æ®...")
        try:
            # è·å–æ‰€æœ‰ç°æœ‰æ–‡æ¡£çš„ID
            existing_data = COLLECTION.get()
            if existing_data and existing_data['ids']:
                COLLECTION.delete(ids=existing_data['ids'])
            logging.info("æˆåŠŸæ¸…ç†å†å²å‘é‡æ•°æ®")
        except Exception as e:
            logging.error(f"æ¸…ç†å†å²æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return f"æ¸…ç†å†å²æ•°æ®å¤±è´¥: {str(e)}", []
        
        # æ¸…ç©ºæ–‡ä»¶å¤„ç†çŠ¶æ€
        file_processor.clear_files()
        
        total_files = len(files)
        processed_results = []
        total_chunks = 0
        
        for idx, file in enumerate(files, 1):
            try:
                file_name = os.path.basename(file.name)
                progress((idx-1)/total_files, desc=f"å¤„ç†æ–‡ä»¶ {idx}/{total_files}: {file_name}")
                
                # æ·»åŠ æ–‡ä»¶åˆ°å¤„ç†å™¨
                file_processor.add_file(file_name)
                
                # å¤„ç†å•ä¸ªæ–‡ä»¶
                text = extract_text(file.name)
                
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800,
                    chunk_overlap=50
                )
                chunks = text_splitter.split_text(text)
                
                if not chunks:
                    raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
                
                # ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†ç¬¦
                doc_id = f"doc_{int(time.time())}_{idx}"
                
                # ç”ŸæˆåµŒå…¥
                embeddings = EMBED_MODEL.encode(chunks)
                
                # å­˜å‚¨å‘é‡ï¼Œæ·»åŠ æ–‡æ¡£æºä¿¡æ¯
                ids = [f"{doc_id}_chunk_{i}" for i in range(len(chunks))]
                metadatas = [{"source": file_name, "doc_id": doc_id} for _ in chunks]
                
                COLLECTION.add(
                    ids=ids,
                    embeddings=embeddings.tolist(),
                    documents=chunks,
                    metadatas=metadatas
                )
                
                # æ›´æ–°å¤„ç†çŠ¶æ€
                total_chunks += len(chunks)
                file_processor.update_status(file_name, "å¤„ç†å®Œæˆ", len(chunks))
                processed_results.append(f"âœ… {file_name}: æˆåŠŸå¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—")
                
            except Exception as e:
                error_msg = str(e)
                logging.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {error_msg}")
                file_processor.update_status(file_name, f"å¤„ç†å¤±è´¥: {error_msg}")
                processed_results.append(f"âŒ {file_name}: å¤„ç†å¤±è´¥ - {error_msg}")
        
        # æ·»åŠ æ€»ç»“ä¿¡æ¯
        summary = f"\næ€»è®¡å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œ{total_chunks} ä¸ªæ–‡æœ¬å—"
        processed_results.append(summary)
        
        # è·å–æ›´æ–°åçš„æ–‡ä»¶åˆ—è¡¨
        file_list = file_processor.get_file_list()
        
        return "\n".join(processed_results), file_list
        
    except Exception as e:
        error_msg = str(e)
        logging.error(f"æ•´ä½“å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}")
        return f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}", []

def stream_answer(question, progress=gr.Progress()):
    """æ”¹è¿›çš„æµå¼é—®ç­”å¤„ç†æµç¨‹"""
    try:
        progress(0.3, desc="ç”Ÿæˆé—®é¢˜åµŒå…¥...")
        query_embedding = EMBED_MODEL.encode([question]).tolist()
        
        progress(0.5, desc="æ£€ç´¢ç›¸å…³å†…å®¹...")
        results = COLLECTION.query(
            query_embeddings=query_embedding,
            n_results=3,
            include=['documents', 'metadatas']
        )
        
        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):
            source = metadata.get('source', 'æœªçŸ¥æ¥æº')
            context_with_sources.append(f"[æ¥æº: {source}]\n{doc}")
        
        context = "\n\n".join(context_with_sources)
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼š
        {context}
        
        é—®é¢˜ï¼š{question}
        è¯·ç”¨ä¸­æ–‡ç»™å‡ºè¯¦ç»†å›ç­”ï¼Œå¹¶åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æºï¼š"""
        
        progress(0.7, desc="ç”Ÿæˆå›ç­”...")
        full_answer = ""
        
        response = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "deepseek-r1:1.5b",
                "prompt": prompt,
                "stream": True
            },
            timeout=120,
            stream=True
        )
        
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line.decode()).get("response", "")
                full_answer += chunk
                yield full_answer, "ç”Ÿæˆå›ç­”ä¸­..."
                
        yield full_answer, "å®Œæˆ!"
        
    except Exception as e:
        yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}", "é‡åˆ°é”™è¯¯"

def query_answer(question, progress=gr.Progress()):
    """é—®ç­”å¤„ç†æµç¨‹"""
    try:
        logging.info(f"æ”¶åˆ°é—®é¢˜ï¼š{question}")
        progress(0.3, desc="ç”Ÿæˆé—®é¢˜åµŒå…¥...")
        # ç”Ÿæˆé—®é¢˜åµŒå…¥
        query_embedding = EMBED_MODEL.encode([question]).tolist()
        
        progress(0.5, desc="æ£€ç´¢ç›¸å…³å†…å®¹...")
        # Chromaæ£€ç´¢
        results = COLLECTION.query(
            query_embeddings=query_embedding,
            n_results=3
        )
        
        # æ„å»ºæç¤ºè¯
        context = "\n".join(results['documents'][0])
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼š
        {context}
        
        é—®é¢˜ï¼š{question}
        è¯·ç”¨ä¸­æ–‡ç»™å‡ºè¯¦ç»†å›ç­”ï¼š"""
        
        progress(0.7, desc="ç”Ÿæˆå›ç­”...")
        # è°ƒç”¨Ollama
        response = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "deepseek-r1:7b",
                "prompt": prompt,
                "stream": False
            },
            timeout=120,  # å»¶é•¿åˆ°2åˆ†é’Ÿ
            headers={'Connection': 'close'}  # æ·»åŠ è¿æ¥å¤´
        )
        response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 
        
        progress(1.0, desc="å®Œæˆ!")
        # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²å¹¶å¤„ç†ç©ºå€¼
        result = response.json()
        return str(result.get("response", "æœªè·å–åˆ°æœ‰æ•ˆå›ç­”"))
    except json.JSONDecodeError:
        return "å“åº”è§£æå¤±è´¥ï¼Œè¯·é‡è¯•"
    except KeyError:
        return "å“åº”æ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡"
    except Exception as e:
        progress(1.0, desc="é‡åˆ°é”™è¯¯")  # ç¡®ä¿è¿›åº¦æ¡å®Œæˆ
        return f"ç³»ç»Ÿé”™è¯¯: {str(e)}"

# ä¿®æ”¹ç•Œé¢å¸ƒå±€éƒ¨åˆ†
with gr.Blocks(
    title="æœ¬åœ°RAGé—®ç­”ç³»ç»Ÿ",
    css="""
    /* å…¨å±€ä¸»é¢˜å˜é‡ */
    :root[data-theme="light"] {
        --text-color: #2c3e50;
        --bg-color: #ffffff;
        --panel-bg: #f8f9fa;
        --border-color: #e9ecef;
        --success-color: #4CAF50;
        --error-color: #f44336;
        --primary-color: #2196F3;
        --secondary-bg: #ffffff;
        --hover-color: #e9ecef;
        --chat-user-bg: #e3f2fd;
        --chat-assistant-bg: #f5f5f5;
    }

    :root[data-theme="dark"] {
        --text-color: #e0e0e0;
        --bg-color: #1a1a1a;
        --panel-bg: #2d2d2d;
        --border-color: #404040;
        --success-color: #81c784;
        --error-color: #e57373;
        --primary-color: #64b5f6;
        --secondary-bg: #2d2d2d;
        --hover-color: #404040;
        --chat-user-bg: #1e3a5f;
        --chat-assistant-bg: #2d2d2d;
    }

    /* å…¨å±€æ ·å¼ */
    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }

    .gradio-container {
        max-width: 1200px !important;
        color: var(--text-color);
        background-color: var(--bg-color);
    }

    /* ä¸»é¢˜åˆ‡æ¢æŒ‰é’® */
    .theme-toggle {
        position: fixed;
        top: 20px;
        right: 20px;
        z-index: 1000;
        padding: 8px 16px;
        border-radius: 20px;
        border: 1px solid var(--border-color);
        background: var(--panel-bg);
        color: var(--text-color);
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 14px;
        display: flex;
        align-items: center;
        gap: 8px;
    }

    .theme-toggle:hover {
        background: var(--hover-color);
    }

    /* é¢æ¿æ ·å¼ */
    .left-panel {
        padding-right: 20px;
        border-right: 1px solid var(--border-color);
        background: var(--bg-color);
    }

    .right-panel {
        height: 100vh;
        background: var(--bg-color);
    }

    /* æ–‡ä»¶åˆ—è¡¨æ ·å¼ */
    .file-list {
        margin-top: 10px;
        padding: 12px;
        background: var(--panel-bg);
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.6;
        border: 1px solid var(--border-color);
    }

    /* ç­”æ¡ˆæ¡†æ ·å¼ */
    .answer-box {
        min-height: 500px !important;
        background: var(--panel-bg);
        border-radius: 8px;
        padding: 16px;
        font-size: 15px;
        line-height: 1.6;
        border: 1px solid var(--border-color);
    }

    /* è¾“å…¥æ¡†æ ·å¼ */
    textarea {
        background: var(--panel-bg) !important;
        color: var(--text-color) !important;
        border: 1px solid var(--border-color) !important;
        border-radius: 8px !important;
        padding: 12px !important;
        font-size: 14px !important;
    }

    /* æŒ‰é’®æ ·å¼ */
    button.primary {
        background: var(--primary-color) !important;
        color: white !important;
        border-radius: 8px !important;
        padding: 8px 16px !important;
        font-weight: 500 !important;
        transition: all 0.3s ease !important;
    }

    button.primary:hover {
        opacity: 0.9;
        transform: translateY(-1px);
    }

    /* æ ‡é¢˜å’Œæ–‡æœ¬æ ·å¼ */
    h1, h2, h3 {
        color: var(--text-color) !important;
        font-weight: 600 !important;
    }

    .footer-note {
        color: var(--text-color);
        opacity: 0.8;
        font-size: 13px;
        margin-top: 12px;
    }

    /* åŠ è½½å’Œè¿›åº¦æ ·å¼ */
    #loading, .progress-text {
        color: var(--text-color);
    }

    /* èŠå¤©è®°å½•æ ·å¼ */
    .chat-container {
        border: 1px solid var(--border-color);
        border-radius: 8px;
        margin-bottom: 16px;
        max-height: 600px;
        overflow-y: auto;
        background: var(--bg-color);
    }

    .chat-message {
        padding: 12px 16px;
        margin: 8px;
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.5;
    }

    .chat-message.user {
        background: var(--chat-user-bg);
        margin-left: 32px;
        border-top-right-radius: 4px;
    }

    .chat-message.assistant {
        background: var(--chat-assistant-bg);
        margin-right: 32px;
        border-top-left-radius: 4px;
    }

    .chat-message .timestamp {
        font-size: 12px;
        color: var(--text-color);
        opacity: 0.7;
        margin-bottom: 4px;
    }

    .chat-message .content {
        white-space: pre-wrap;
    }

    /* æŒ‰é’®ç»„æ ·å¼ */
    .button-row {
        display: flex;
        gap: 8px;
        margin-top: 8px;
    }

    .clear-button {
        background: var(--error-color) !important;
    }
    """
) as demo:
    gr.Markdown("# ğŸ§  æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    
    with gr.Row():
        # å·¦ä¾§æ“ä½œé¢æ¿
        with gr.Column(scale=1, elem_classes="left-panel"):
            gr.Markdown("## ğŸ“‚ æ–‡æ¡£å¤„ç†åŒº")
            with gr.Group():
                file_input = gr.File(
                    label="ä¸Šä¼ PDFæ–‡æ¡£",
                    file_types=[".pdf"],
                    file_count="multiple"
                )
                upload_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                upload_status = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    interactive=False,
                    lines=2
                )
                file_list = gr.Textbox(
                    label="å·²å¤„ç†æ–‡ä»¶",
                    interactive=False,
                    lines=3,
                    elem_classes="file-list"
                )

        # å³ä¾§å¯¹è¯åŒº
        with gr.Column(scale=3, elem_classes="right-panel"):
            gr.Markdown("## ğŸ“ å¯¹è¯è®°å½•")
            
            # å¯¹è¯è®°å½•æ˜¾ç¤ºåŒº
            chatbot = gr.Chatbot(
                label="å¯¹è¯å†å²",
                height=500,
                elem_classes="chat-container",
                show_label=False
            )
            
            # é—®é¢˜è¾“å…¥åŒº
            with gr.Group():
                question_input = gr.Textbox(
                    label="è¾“å…¥é—®é¢˜",
                    lines=3,
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                    elem_id="question-input"
                )
                with gr.Row():
                    ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary", scale=2)
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary", elem_classes="clear-button", scale=1)
                status_display = gr.HTML("", elem_id="status-display")
            
            gr.Markdown("""
            <div class="footer-note">
                *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
            </div>
            """)

    # è°ƒæ•´åçš„åŠ è½½æç¤º
    gr.HTML("""
    <div id="loading" style="text-align:center;padding:20px;">
        <h3>ğŸ”„ ç³»ç»Ÿåˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™...</h3>
    </div>
    """)

    # è¿›åº¦æ˜¾ç¤ºç»„ä»¶è°ƒæ•´åˆ°å·¦ä¾§é¢æ¿ä¸‹æ–¹
    with gr.Row(visible=False) as progress_row:
        gr.HTML("""
        <div class="progress-text">
            <span>å½“å‰è¿›åº¦ï¼š</span>
            <span id="current-step" style="color: #2b6de3;">åˆå§‹åŒ–...</span>
            <span id="progress-percent" style="margin-left:15px;color: #e32b2b;">0%</span>
        </div>
        """)

    def clear_chat_history():
        return [], ""  # æ¸…ç©ºå¯¹è¯å†å²å’Œè¾“å…¥æ¡†

    # ä¿®æ”¹é—®ç­”å¤„ç†å‡½æ•°
    def process_chat(question, history):
        if not question:
            return history, ""
        
        history = history or []
        history.append([question, None])
        
        try:
            for response, status in stream_answer(question):
                if status != "é‡åˆ°é”™è¯¯":
                    history[-1][1] = response
                    yield history, ""
                else:
                    history[-1][1] = f"âŒ {response}"
                    yield history, ""
        except Exception as e:
            history[-1][1] = f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}"
            yield history, ""

    # æ›´æ–°äº‹ä»¶å¤„ç†
    ask_btn.click(
        fn=process_chat,
        inputs=[question_input, chatbot],
        outputs=[chatbot, question_input],
        show_progress=False
    ).then(
        fn=lambda: "",
        outputs=status_display
    )

    clear_btn.click(
        fn=clear_chat_history,
        outputs=[chatbot, question_input],
        show_progress=False
    )

    # æ·»åŠ æ–‡ä»¶å¤„ç†æŒ‰é’®äº‹ä»¶
    upload_btn.click(
        fn=process_multiple_pdfs,
        inputs=file_input,
        outputs=[upload_status, file_list]
    )

# ä¿®æ”¹JavaScriptæ³¨å…¥éƒ¨åˆ†
demo._js = """
function gradioApp() {
    // è®¾ç½®é»˜è®¤ä¸»é¢˜ä¸ºæš—è‰²
    document.documentElement.setAttribute('data-theme', 'dark');
    
    const observer = new MutationObserver((mutations) => {
        document.getElementById("loading").style.display = "none";
        const progress = document.querySelector('.progress-text');
        if (progress) {
            const percent = document.querySelector('.progress > div')?.innerText || '';
            const step = document.querySelector('.progress-description')?.innerText || '';
            document.getElementById('current-step').innerText = step;
            document.getElementById('progress-percent').innerText = percent;
        }
    });
    observer.observe(document.body, {childList: true, subtree: true});
}

function toggleTheme() {
    const root = document.documentElement;
    const currentTheme = root.getAttribute('data-theme');
    const newTheme = currentTheme === 'light' ? 'dark' : 'light';
    root.setAttribute('data-theme', newTheme);
}

// åˆå§‹åŒ–ä¸»é¢˜
document.addEventListener('DOMContentLoaded', () => {
    document.documentElement.setAttribute('data-theme', 'dark');
});
"""

# ä¿®æ”¹ç«¯å£æ£€æŸ¥å‡½æ•°
def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0  # æ›´å¯é çš„æ£€æµ‹æ–¹å¼

def check_environment():
    """ç¯å¢ƒä¾èµ–æ£€æŸ¥"""
    try:
        # æ·»åŠ æ¨¡å‹å­˜åœ¨æ€§æ£€æŸ¥
        model_check = session.post(
            "http://localhost:11434/api/show",
            json={"name": "deepseek-r1:7b"},
            timeout=10
        )
        if model_check.status_code != 200:
            print("æ¨¡å‹æœªåŠ è½½ï¼è¯·å…ˆæ‰§è¡Œï¼š")
            print("ollama pull deepseek-r1:7b")
            return False
            
        # åŸæœ‰æ£€æŸ¥ä¿æŒä¸å˜...
        response = session.get(
            "http://localhost:11434/api/tags",
            proxies={"http": None, "https": None},  # ç¦ç”¨ä»£ç†
            timeout=5
        )
        if response.status_code != 200:
            print("OllamaæœåŠ¡å¼‚å¸¸ï¼Œè¿”å›çŠ¶æ€ç :", response.status_code)
            return False
        return True
    except Exception as e:
        print("Ollamaè¿æ¥å¤±è´¥:", str(e))
        return False

# æ–¹æ¡ˆ2ï¼šç¦ç”¨æµè§ˆå™¨ç¼“å­˜ï¼ˆæ·»åŠ metaæ ‡ç­¾ï¼‰
gr.HTML("""
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Expires" content="0">
""")

# æ¢å¤ä¸»ç¨‹åºå¯åŠ¨éƒ¨åˆ†
if __name__ == "__main__":
    if not check_environment():
        exit(1)
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)
    
    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)
        
    try:
        ollama_check = session.get("http://localhost:11434", timeout=5)
        if ollama_check.status_code != 200:
            print("OllamaæœåŠ¡æœªæ­£å¸¸å¯åŠ¨ï¼")
            print("è¯·å…ˆæ‰§è¡Œï¼šollama serve å¯åŠ¨æœåŠ¡")
            exit(1)
            
        webbrowser.open(f"http://127.0.0.1:{selected_port}")
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}")

