import gradio as gr
from pdfminer.high_level import extract_text_to_fp
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import requests
import json
from io import StringIO
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import socket
import webbrowser
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ è¶…æ—¶è®¾ç½®
import requests
requests.adapters.DEFAULT_RETRIES = 3  # å¢åŠ é‡è¯•æ¬¡æ•°

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ç¯å¢ƒå˜é‡è®¾ç½®
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # ç¦ç”¨oneDNNä¼˜åŒ–

# åœ¨æ–‡ä»¶æœ€å¼€å¤´æ·»åŠ ä»£ç†é…ç½®
import os
os.environ['NO_PROXY'] = 'localhost,127.0.0.1'  # æ–°å¢ä»£ç†ç»•è¿‡è®¾ç½®

# åˆå§‹åŒ–ç»„ä»¶
EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
CHROMA_CLIENT = chromadb.PersistentClient(
    path="./chroma_db",
    settings=chromadb.Settings(anonymized_telemetry=False)
)
COLLECTION = CHROMA_CLIENT.get_or_create_collection("rag_docs")

logging.basicConfig(level=logging.INFO)

print("Gradio version:", gr.__version__)  # æ·»åŠ ç‰ˆæœ¬è¾“å‡º

# åœ¨åˆå§‹åŒ–ç»„ä»¶åæ·»åŠ ï¼š
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
session.mount('http://', HTTPAdapter(max_retries=retries))

def extract_text(filepath):
    """æ”¹è¿›çš„PDFæ–‡æœ¬æå–æ–¹æ³•"""
    output = StringIO()
    with open(filepath, 'rb') as file:
        extract_text_to_fp(file, output)
    return output.getvalue()

def process_pdf(file, progress=gr.Progress()):
    """PDFå¤„ç†å…¨æµç¨‹"""
    try:
        progress(0.2, desc="è§£æPDF...")
        text = extract_text(file.name)
        
        progress(0.4, desc="åˆ†å‰²æ–‡æœ¬...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=50
        )
        chunks = text_splitter.split_text(text)
        
        progress(0.6, desc="ç”ŸæˆåµŒå…¥...")
        embeddings = EMBED_MODEL.encode(chunks)
        
        progress(0.8, desc="å­˜å‚¨å‘é‡...")
        # æ¸…ç©ºç°æœ‰æ•°æ®çš„æ­£ç¡®æ–¹å¼
        existing_ids = COLLECTION.get()['ids']
        if existing_ids:
            COLLECTION.delete(ids=existing_ids)
        # å­˜å…¥æ–°æ•°æ®
        ids = [str(i) for i in range(len(chunks))]
        COLLECTION.add(
            ids=ids,
            embeddings=embeddings.tolist(),
            documents=chunks
        )
        
        progress(1.0, desc="å®Œæˆ!")
        return "PDFå¤„ç†å®Œæˆï¼Œå·²å­˜å‚¨ {} ä¸ªæ–‡æœ¬å—".format(len(chunks))
    except Exception as e:
        return f"å¤„ç†å¤±è´¥: {str(e)}"

def stream_answer(question, progress=gr.Progress()):
    """æµå¼é—®ç­”å¤„ç†æµç¨‹"""
    try:
        progress(0.3, desc="ç”Ÿæˆé—®é¢˜åµŒå…¥...")
        query_embedding = EMBED_MODEL.encode([question]).tolist()
        
        progress(0.5, desc="æ£€ç´¢ç›¸å…³å†…å®¹...")
        results = COLLECTION.query(
            query_embeddings=query_embedding,
            n_results=3
        )
        
        context = "\n".join(results['documents'][0])
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼š
        {context}
        
        é—®é¢˜ï¼š{question}
        è¯·ç”¨ä¸­æ–‡ç»™å‡ºè¯¦ç»†å›ç­”ï¼š"""
        
        progress(0.7, desc="ç”Ÿæˆå›ç­”...")
        full_answer = ""
        
        # æµå¼è¯·æ±‚
        response = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "deepseek-r1:1.5b",
                "prompt": prompt,
                "stream": True  # å¯ç”¨æµå¼
            },
            timeout=120,
            stream=True
        )
        
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line.decode()).get("response", "")
                full_answer += chunk
                yield full_answer, "ç”Ÿæˆå›ç­”ä¸­..."
                
        yield full_answer, "å®Œæˆ!"
        
    except Exception as e:
        yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}", "é‡åˆ°é”™è¯¯"

def query_answer(question, progress=gr.Progress()):
    """é—®ç­”å¤„ç†æµç¨‹"""
    try:
        logging.info(f"æ”¶åˆ°é—®é¢˜ï¼š{question}")
        progress(0.3, desc="ç”Ÿæˆé—®é¢˜åµŒå…¥...")
        # ç”Ÿæˆé—®é¢˜åµŒå…¥
        query_embedding = EMBED_MODEL.encode([question]).tolist()
        
        progress(0.5, desc="æ£€ç´¢ç›¸å…³å†…å®¹...")
        # Chromaæ£€ç´¢
        results = COLLECTION.query(
            query_embeddings=query_embedding,
            n_results=3
        )
        
        # æ„å»ºæç¤ºè¯
        context = "\n".join(results['documents'][0])
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼š
        {context}
        
        é—®é¢˜ï¼š{question}
        è¯·ç”¨ä¸­æ–‡ç»™å‡ºè¯¦ç»†å›ç­”ï¼š"""
        
        progress(0.7, desc="ç”Ÿæˆå›ç­”...")
        # è°ƒç”¨Ollama
        response = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "deepseek-r1:1.5b",
                "prompt": prompt,
                "stream": False
            },
            timeout=120,  # å»¶é•¿åˆ°2åˆ†é’Ÿ
            headers={'Connection': 'close'}  # æ·»åŠ è¿æ¥å¤´
        )
        response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 
        
        progress(1.0, desc="å®Œæˆ!")
        # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²å¹¶å¤„ç†ç©ºå€¼
        result = response.json()
        return str(result.get("response", "æœªè·å–åˆ°æœ‰æ•ˆå›ç­”"))
    except json.JSONDecodeError:
        return "å“åº”è§£æå¤±è´¥ï¼Œè¯·é‡è¯•"
    except KeyError:
        return "å“åº”æ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡"
    except Exception as e:
        progress(1.0, desc="é‡åˆ°é”™è¯¯")  # ç¡®ä¿è¿›åº¦æ¡å®Œæˆ
        return f"ç³»ç»Ÿé”™è¯¯: {str(e)}"

# ä¿®æ”¹ç•Œé¢å¸ƒå±€éƒ¨åˆ†
with gr.Blocks(
    title="æœ¬åœ°RAGé—®ç­”ç³»ç»Ÿ",
    css="""
    .gradio-container {max-width: 1200px !important}
    .answer-box {min-height: 500px !important;}
    .left-panel {padding-right: 20px; border-right: 1px solid #eee;}
    .right-panel {height: 100vh;}
    """
) as demo:
    gr.Markdown("# ğŸ§  æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    
    with gr.Row():
        # å·¦ä¾§æ“ä½œé¢æ¿
        with gr.Column(scale=1, elem_classes="left-panel"):
            gr.Markdown("## ğŸ“‚ æ–‡æ¡£å¤„ç†åŒº")
            with gr.Group():
                file_input = gr.File(label="ä¸Šä¼ PDFæ–‡æ¡£", file_types=[".pdf"])
                upload_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                upload_status = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False)
            
            gr.Markdown("## â“ æé—®åŒº")
            with gr.Group():
                question_input = gr.Textbox(
                    label="è¾“å…¥é—®é¢˜",
                    lines=4,
                    placeholder="ä¾‹å¦‚ï¼šæœ¬æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                    elem_id="question-input"
                )
                ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary")
                status_display = gr.HTML("", elem_id="status-display")

        # å³ä¾§ç­”æ¡ˆæ˜¾ç¤ºåŒº
        with gr.Column(scale=3, elem_classes="right-panel"):
            gr.Markdown("## ğŸ“ ç­”æ¡ˆå±•ç¤º")
            answer_output = gr.Textbox(
                label="æ™ºèƒ½å›ç­”",
                interactive=False,
                lines=25,
                elem_classes="answer-box",
                autoscroll=True,
                show_copy_button=True
            )
            gr.Markdown("""
            <div class="footer-note">
                *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
            </div>
            """)

    # è°ƒæ•´åçš„åŠ è½½æç¤º
    gr.HTML("""
    <div id="loading" style="text-align:center;padding:20px;">
        <h3>ğŸ”„ ç³»ç»Ÿåˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™...</h3>
    </div>
    """)

    # è¿›åº¦æ˜¾ç¤ºç»„ä»¶è°ƒæ•´åˆ°å·¦ä¾§é¢æ¿ä¸‹æ–¹
    with gr.Row(visible=False) as progress_row:
        gr.HTML("""
        <div class="progress-text">
            <span>å½“å‰è¿›åº¦ï¼š</span>
            <span id="current-step" style="color: #2b6de3;">åˆå§‹åŒ–...</span>
            <span id="progress-percent" style="margin-left:15px;color: #e32b2b;">0%</span>
        </div>
        """)

    # åœ¨ç•Œé¢ç»„ä»¶å®šä¹‰ä¹‹åæ·»åŠ æŒ‰é’®äº‹ä»¶
    ask_btn.click(
        fn=stream_answer,
        inputs=question_input,
        outputs=[answer_output, status_display],
        show_progress="hidden"
    )

    upload_btn.click(
        fn=process_pdf,
        inputs=file_input,
        outputs=upload_status
    )

# ä¿®æ”¹JavaScriptæ³¨å…¥éƒ¨åˆ†ä¸ºå…¼å®¹å†™æ³•
demo._js = """
function gradioApp() {
    const observer = new MutationObserver((mutations) => {
        document.getElementById("loading").style.display = "none";
        const progress = document.querySelector('.progress-text');
        if (progress) {
            const percent = document.querySelector('.progress > div')?.innerText || '';
            const step = document.querySelector('.progress-description')?.innerText || '';
            document.getElementById('current-step').innerText = step;
            document.getElementById('progress-percent').innerText = percent;
        }
    });
    observer.observe(document.body, {childList: true, subtree: true});
}
"""

# ä¿®æ”¹ç«¯å£æ£€æŸ¥å‡½æ•°
def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0  # æ›´å¯é çš„æ£€æµ‹æ–¹å¼

def check_environment():
    """ç¯å¢ƒä¾èµ–æ£€æŸ¥"""
    try:
        # æ·»åŠ æ¨¡å‹å­˜åœ¨æ€§æ£€æŸ¥
        model_check = session.post(
            "http://localhost:11434/api/show",
            json={"name": "deepseek-r1:7b"},
            timeout=10
        )
        if model_check.status_code != 200:
            print("æ¨¡å‹æœªåŠ è½½ï¼è¯·å…ˆæ‰§è¡Œï¼š")
            print("ollama pull deepseek-r1:7b")
            return False
            
        # åŸæœ‰æ£€æŸ¥ä¿æŒä¸å˜...
        response = session.get(
            "http://localhost:11434/api/tags",
            proxies={"http": None, "https": None},  # ç¦ç”¨ä»£ç†
            timeout=5
        )
        if response.status_code != 200:
            print("OllamaæœåŠ¡å¼‚å¸¸ï¼Œè¿”å›çŠ¶æ€ç :", response.status_code)
            return False
        return True
    except Exception as e:
        print("Ollamaè¿æ¥å¤±è´¥:", str(e))
        return False

# æ–¹æ¡ˆ2ï¼šç¦ç”¨æµè§ˆå™¨ç¼“å­˜ï¼ˆæ·»åŠ metaæ ‡ç­¾ï¼‰
gr.HTML("""
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Expires" content="0">
""")

# æ¢å¤ä¸»ç¨‹åºå¯åŠ¨éƒ¨åˆ†
if __name__ == "__main__":
    if not check_environment():
        exit(1)
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)
    
    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)
        
    try:
        ollama_check = session.get("http://localhost:11434", timeout=5)
        if ollama_check.status_code != 200:
            print("OllamaæœåŠ¡æœªæ­£å¸¸å¯åŠ¨ï¼")
            print("è¯·å…ˆæ‰§è¡Œï¼šollama serve å¯åŠ¨æœåŠ¡")
            exit(1)
            
        webbrowser.open(f"http://127.0.0.1:{selected_port}")
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}")

