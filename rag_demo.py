import gradio as gr
from pdfminer.high_level import extract_text_to_fp
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import requests
import json
from io import StringIO
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import socket
import webbrowser
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time
from datetime import datetime
import hashlib
import re
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
SERPAPI_KEY = os.getenv("SERPAPI_KEY")  # åœ¨.envä¸­è®¾ç½® SERPAPI_KEY
SEARCH_ENGINE = "google"  # å¯æ ¹æ®éœ€è¦æ”¹ä¸ºå…¶ä»–æœç´¢å¼•æ“

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ è¶…æ—¶è®¾ç½®
import requests
requests.adapters.DEFAULT_RETRIES = 3  # å¢åŠ é‡è¯•æ¬¡æ•°

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ç¯å¢ƒå˜é‡è®¾ç½®
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # ç¦ç”¨oneDNNä¼˜åŒ–

# åœ¨æ–‡ä»¶æœ€å¼€å¤´æ·»åŠ ä»£ç†é…ç½®
import os
os.environ['NO_PROXY'] = 'localhost,127.0.0.1'  # æ–°å¢ä»£ç†ç»•è¿‡è®¾ç½®

# åˆå§‹åŒ–ç»„ä»¶
EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
CHROMA_CLIENT = chromadb.PersistentClient(
    path="./chroma_db",
    settings=chromadb.Settings(anonymized_telemetry=False)
)
COLLECTION = CHROMA_CLIENT.get_or_create_collection("rag_docs")

logging.basicConfig(level=logging.INFO)

print("Gradio version:", gr.__version__)  # æ·»åŠ ç‰ˆæœ¬è¾“å‡º

# åœ¨åˆå§‹åŒ–ç»„ä»¶åæ·»åŠ ï¼š
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
session.mount('http://', HTTPAdapter(max_retries=retries))

#########################################
# SerpAPI ç½‘ç»œæŸ¥è¯¢åŠå‘é‡åŒ–å¤„ç†å‡½æ•°
#########################################
def serpapi_search(query: str, num_results: int = 5) -> list:
    """
    æ‰§è¡Œ SerpAPI æœç´¢ï¼Œå¹¶è¿”å›è§£æåçš„ç»“æ„åŒ–ç»“æœ
    """
    if not SERPAPI_KEY:
        raise ValueError("æœªè®¾ç½® SERPAPI_KEY ç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ API å¯†é’¥ã€‚")
    try:
        params = {
            "engine": SEARCH_ENGINE,
            "q": query,
            "api_key": SERPAPI_KEY,
            "num": num_results,
            "hl": "zh-CN",  # ä¸­æ–‡ç•Œé¢
            "gl": "cn"
        }
        response = requests.get("https://serpapi.com/search", params=params, timeout=15)
        response.raise_for_status()
        search_data = response.json()
        return _parse_serpapi_results(search_data)
    except Exception as e:
        logging.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
        return []

def _parse_serpapi_results(data: dict) -> list:
    """è§£æ SerpAPI è¿”å›çš„åŸå§‹æ•°æ®"""
    results = []
    if "organic_results" in data:
        for item in data["organic_results"]:
            result = {
                "title": item.get("title"),
                "url": item.get("link"),
                "snippet": item.get("snippet"),
                "timestamp": item.get("date")  # è‹¥æœ‰æ—¶é—´ä¿¡æ¯ï¼Œå¯é€‰
            }
            results.append(result)
    # å¦‚æœæœ‰çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥æ·»åŠ ç½®é¡¶ï¼ˆå¯é€‰ï¼‰
    if "knowledge_graph" in data:
        kg = data["knowledge_graph"]
        results.insert(0, {
            "title": kg.get("title"),
            "url": kg.get("source", {}).get("link", ""),
            "snippet": kg.get("description"),
            "source": "knowledge_graph"
        })
    return results

def update_web_results(query: str, num_results: int = 5) -> list:
    """
    åŸºäº SerpAPI æœç´¢ç»“æœï¼Œå‘é‡åŒ–å¹¶å­˜å‚¨åˆ° ChromaDB
    ä¸ºç½‘ç»œç»“æœæ·»åŠ å…ƒæ•°æ®ï¼ŒID æ ¼å¼ä¸º "web_{index}"
    """
    results = serpapi_search(query, num_results)
    if not results:
        return []
    # åˆ é™¤æ—§çš„ç½‘ç»œæœç´¢ç»“æœ
    existing_ids = COLLECTION.get()['ids']
    web_ids = [doc_id for doc_id in existing_ids if doc_id.startswith("web_")]
    if web_ids:
        COLLECTION.delete(ids=web_ids)
    docs = []
    metadatas = []
    ids = []
    for idx, res in enumerate(results):
        text = f"æ ‡é¢˜ï¼š{res.get('title', '')}\næ‘˜è¦ï¼š{res.get('snippet', '')}"
        docs.append(text)
        meta = {"source": "web", "url": res.get("url", ""), "title": res.get("title")}
        meta["content_hash"] = hashlib.md5(text.encode()).hexdigest()[:8]
        metadatas.append(meta)
        ids.append(f"web_{idx}")
    embeddings = EMBED_MODEL.encode(docs)
    COLLECTION.add(ids=ids, embeddings=embeddings.tolist(), documents=docs, metadatas=metadatas)
    return results

# æ£€æŸ¥æ˜¯å¦é…ç½®äº†SERPAPI_KEY
def check_serpapi_key():
    """æ£€æŸ¥æ˜¯å¦é…ç½®äº†SERPAPI_KEY"""
    return SERPAPI_KEY is not None and SERPAPI_KEY.strip() != ""

# æ·»åŠ æ–‡ä»¶å¤„ç†çŠ¶æ€è·Ÿè¸ª
class FileProcessor:
    def __init__(self):
        self.processed_files = {}  # å­˜å‚¨å·²å¤„ç†æ–‡ä»¶çš„çŠ¶æ€
        
    def clear_files(self):
        """æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶è®°å½•"""
        self.processed_files = {}
        
    def add_file(self, file_name):
        self.processed_files[file_name] = {
            'status': 'ç­‰å¾…å¤„ç†',
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'chunks': 0
        }
        
    def update_status(self, file_name, status, chunks=None):
        if file_name in self.processed_files:
            self.processed_files[file_name]['status'] = status
            if chunks is not None:
                self.processed_files[file_name]['chunks'] = chunks
                
    def get_file_list(self):
        return [
            f"ğŸ“„ {fname} | {info['status']}"
            for fname, info in self.processed_files.items()
        ]

file_processor = FileProcessor()

#########################################
# çŸ›ç›¾æ£€æµ‹å‡½æ•°
#########################################
def detect_conflicts(sources):
    """ç²¾å‡†çŸ›ç›¾æ£€æµ‹ç®—æ³•"""
    key_facts = {}
    for item in sources:
        facts = extract_facts(item['text'] if 'text' in item else item.get('excerpt', ''))
        for fact, value in facts.items():
            if fact in key_facts:
                if key_facts[fact] != value:
                    return True
            else:
                key_facts[fact] = value
    return False

def extract_facts(text):
    """ä»æ–‡æœ¬æå–å…³é”®äº‹å®ï¼ˆç¤ºä¾‹é€»è¾‘ï¼‰"""
    facts = {}
    # æå–æ•°å€¼å‹äº‹å®
    numbers = re.findall(r'\b\d{4}å¹´|\b\d+%', text)
    if numbers:
        facts['å…³é”®æ•°å€¼'] = numbers
    # æå–æŠ€æœ¯æœ¯è¯­
    if "äº§ä¸šå›¾è°±" in text:
        facts['æŠ€æœ¯æ–¹æ³•'] = list(set(re.findall(r'[A-Za-z]+æ¨¡å‹|[A-Z]{2,}ç®—æ³•', text)))
    return facts

def evaluate_source_credibility(source):
    """è¯„ä¼°æ¥æºå¯ä¿¡åº¦"""
    credibility_scores = {
        "gov.cn": 0.9,
        "edu.cn": 0.85,
        "weixin": 0.7,
        "zhihu": 0.6,
        "baidu": 0.5
    }
    
    url = source.get('url', '')
    if not url:
        return 0.5  # é»˜è®¤ä¸­ç­‰å¯ä¿¡åº¦
    
    domain_match = re.search(r'//([^/]+)', url)
    if not domain_match:
        return 0.5
    
    domain = domain_match.group(1)
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å·²çŸ¥åŸŸå
    for known_domain, score in credibility_scores.items():
        if known_domain in domain:
            return score
    
    return 0.5  # é»˜è®¤ä¸­ç­‰å¯ä¿¡åº¦

def extract_text(filepath):
    """æ”¹è¿›çš„PDFæ–‡æœ¬æå–æ–¹æ³•"""
    output = StringIO()
    with open(filepath, 'rb') as file:
        extract_text_to_fp(file, output)
    return output.getvalue()

def process_multiple_pdfs(files, progress=gr.Progress()):
    """å¤„ç†å¤šä¸ªPDFæ–‡ä»¶"""
    if not files:
        return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„PDFæ–‡ä»¶", []
    
    try:
        # æ¸…ç©ºå‘é‡æ•°æ®åº“
        progress(0.1, desc="æ¸…ç†å†å²æ•°æ®...")
        try:
            # è·å–æ‰€æœ‰ç°æœ‰æ–‡æ¡£çš„ID
            existing_data = COLLECTION.get()
            if existing_data and existing_data['ids']:
                COLLECTION.delete(ids=existing_data['ids'])
            logging.info("æˆåŠŸæ¸…ç†å†å²å‘é‡æ•°æ®")
        except Exception as e:
            logging.error(f"æ¸…ç†å†å²æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return f"æ¸…ç†å†å²æ•°æ®å¤±è´¥: {str(e)}", []
        
        # æ¸…ç©ºæ–‡ä»¶å¤„ç†çŠ¶æ€
        file_processor.clear_files()
        
        total_files = len(files)
        processed_results = []
        total_chunks = 0
        
        for idx, file in enumerate(files, 1):
            try:
                file_name = os.path.basename(file.name)
                progress((idx-1)/total_files, desc=f"å¤„ç†æ–‡ä»¶ {idx}/{total_files}: {file_name}")
                
                # æ·»åŠ æ–‡ä»¶åˆ°å¤„ç†å™¨
                file_processor.add_file(file_name)
                
                # å¤„ç†å•ä¸ªæ–‡ä»¶
                text = extract_text(file.name)
                
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800,
                    chunk_overlap=50
                )
                chunks = text_splitter.split_text(text)
                
                if not chunks:
                    raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
                
                # ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†ç¬¦
                doc_id = f"doc_{int(time.time())}_{idx}"
                
                # ç”ŸæˆåµŒå…¥
                embeddings = EMBED_MODEL.encode(chunks)
                
                # å­˜å‚¨å‘é‡ï¼Œæ·»åŠ æ–‡æ¡£æºä¿¡æ¯
                ids = [f"{doc_id}_chunk_{i}" for i in range(len(chunks))]
                metadatas = [{"source": file_name, "doc_id": doc_id} for _ in chunks]
                
                COLLECTION.add(
                    ids=ids,
                    embeddings=embeddings.tolist(),
                    documents=chunks,
                    metadatas=metadatas
                )
                
                # æ›´æ–°å¤„ç†çŠ¶æ€
                total_chunks += len(chunks)
                file_processor.update_status(file_name, "å¤„ç†å®Œæˆ", len(chunks))
                processed_results.append(f"âœ… {file_name}: æˆåŠŸå¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—")
                
            except Exception as e:
                error_msg = str(e)
                logging.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {error_msg}")
                file_processor.update_status(file_name, f"å¤„ç†å¤±è´¥: {error_msg}")
                processed_results.append(f"âŒ {file_name}: å¤„ç†å¤±è´¥ - {error_msg}")
        
        # æ·»åŠ æ€»ç»“ä¿¡æ¯
        summary = f"\næ€»è®¡å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œ{total_chunks} ä¸ªæ–‡æœ¬å—"
        processed_results.append(summary)
        
        # è·å–æ›´æ–°åçš„æ–‡ä»¶åˆ—è¡¨
        file_list = file_processor.get_file_list()
        
        return "\n".join(processed_results), file_list
        
    except Exception as e:
        error_msg = str(e)
        logging.error(f"æ•´ä½“å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}")
        return f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}", []

def stream_answer(question, enable_web_search=False, progress=gr.Progress()):
    """æ”¹è¿›çš„æµå¼é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢"""
    try:
        # å¦‚æœå¯ç”¨äº†è”ç½‘æœç´¢ï¼Œå…ˆè¿›è¡Œç½‘ç»œæœç´¢
        if enable_web_search:
            if not check_serpapi_key():
                yield "âš ï¸ è”ç½‘åŠŸèƒ½å¯ç”¨å¤±è´¥ï¼šæœªé…ç½®SERPAPI_KEYã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­æ·»åŠ æ‚¨çš„APIå¯†é’¥ã€‚", "é”™è¯¯"
                return
                
            progress(0.3, desc="æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢...")
            try:
                web_results = update_web_results(question)
                if not web_results:
                    progress(0.4, desc="ç½‘ç»œæœç´¢æœªè¿”å›ç»“æœï¼Œç»§ç»­ä½¿ç”¨æœ¬åœ°çŸ¥è¯†...")
            except Exception as e:
                progress(0.4, desc="ç½‘ç»œæœç´¢å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°çŸ¥è¯†...")
                logging.error(f"ç½‘ç»œæœç´¢é”™è¯¯: {str(e)}")
                yield f"ç½‘ç»œæœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}ï¼Œå°†ä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“å›ç­”", "æœç´¢å¤±è´¥"
        
        progress(0.5, desc="ç”Ÿæˆé—®é¢˜åµŒå…¥...")
        query_embedding = EMBED_MODEL.encode([question]).tolist()
        
        progress(0.6, desc="æ£€ç´¢ç›¸å…³å†…å®¹...")
        results = COLLECTION.query(
            query_embeddings=query_embedding,
            n_results=5,  # å¢åŠ æ£€ç´¢ç»“æœæ•°é‡
            include=['documents', 'metadatas']
        )
        
        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        sources_for_conflict_detection = []
        
        for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):
            source_type = metadata.get('source', 'æœ¬åœ°æ–‡æ¡£')
            
            source_item = {
                'text': doc,
                'type': source_type
            }
            
            if source_type == 'web':
                url = metadata.get('url', 'æœªçŸ¥URL')
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                context_with_sources.append(f"[ç½‘ç»œæ¥æº: {title}] (URL: {url})\n{doc}")
                source_item['url'] = url
                source_item['title'] = title
            else:
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                context_with_sources.append(f"[æœ¬åœ°æ–‡æ¡£: {source}]\n{doc}")
                source_item['source'] = source
            
            sources_for_conflict_detection.append(source_item)
        
        # æ£€æµ‹çŸ›ç›¾
        conflict_detected = detect_conflicts(sources_for_conflict_detection)
        
        # è·å–å¯ä¿¡æº
        if conflict_detected:
            credible_sources = [s for s in sources_for_conflict_detection 
                               if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]
        
        context = "\n\n".join(context_with_sources)
        
        # æ·»åŠ æ—¶é—´æ•æ„Ÿæ£€æµ‹
        time_sensitive = any(word in question for word in ["æœ€æ–°", "ä»Šå¹´", "å½“å‰", "æœ€è¿‘", "åˆšåˆš"])
        
        prompt_template = """åŸºäºä»¥ä¸‹{context_type}ï¼š
        {context}
        
        é—®é¢˜ï¼š{question}
        è¯·ç”¨ä¸­æ–‡ç»™å‡ºè¯¦ç»†å›ç­”ï¼Œå¹¶åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æºã€‚{time_note}{conflict_note}"""
        
        prompt = prompt_template.format(
            context_type="æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œæœç´¢ç»“æœ" if enable_web_search else "æœ¬åœ°æ–‡æ¡£",
            context=context,
            question=question,
            time_note="æ³¨æ„è¿™æ˜¯æ—¶é—´æ•æ„Ÿçš„é—®é¢˜ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨æœ€æ–°ä¿¡æ¯ã€‚" if time_sensitive and enable_web_search else "",
            conflict_note="\næ³¨æ„ï¼šæ£€æµ‹åˆ°ä¿¡æ¯æºä¹‹é—´å¯èƒ½å­˜åœ¨çŸ›ç›¾ï¼Œè¯·åœ¨å›ç­”ä¸­æ˜ç¡®æŒ‡å‡ºä¸åŒæ¥æºçš„å·®å¼‚ã€‚" if conflict_detected else ""
        )
        
        progress(0.7, desc="ç”Ÿæˆå›ç­”...")
        full_answer = ""
        
        response = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "deepseek-r1:1.5b",
                "prompt": prompt,
                "stream": True
            },
            timeout=120,
            stream=True
        )
        
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line.decode()).get("response", "")
                full_answer += chunk
                yield full_answer, "ç”Ÿæˆå›ç­”ä¸­..."
                
        yield full_answer, "å®Œæˆ!"
        
    except Exception as e:
        yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}", "é‡åˆ°é”™è¯¯"

def query_answer(question, enable_web_search=False, progress=gr.Progress()):
    """é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢"""
    try:
        logging.info(f"æ”¶åˆ°é—®é¢˜ï¼š{question}ï¼Œè”ç½‘çŠ¶æ€ï¼š{enable_web_search}")
        
        # å¦‚æœå¯ç”¨äº†è”ç½‘æœç´¢ï¼Œå…ˆè¿›è¡Œç½‘ç»œæœç´¢
        if enable_web_search:
            if not check_serpapi_key():
                return "âš ï¸ è”ç½‘åŠŸèƒ½å¯ç”¨å¤±è´¥ï¼šæœªé…ç½®SERPAPI_KEYã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­æ·»åŠ æ‚¨çš„APIå¯†é’¥ã€‚"
                
            progress(0.2, desc="æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢...")
            try:
                web_results = update_web_results(question)
                if not web_results:
                    progress(0.3, desc="ç½‘ç»œæœç´¢æœªè¿”å›ç»“æœï¼Œç»§ç»­ä½¿ç”¨æœ¬åœ°çŸ¥è¯†...")
            except Exception as e:
                progress(0.3, desc="ç½‘ç»œæœç´¢å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°çŸ¥è¯†...")
                logging.error(f"ç½‘ç»œæœç´¢é”™è¯¯: {str(e)}")
        
        progress(0.4, desc="ç”Ÿæˆé—®é¢˜åµŒå…¥...")
        # ç”Ÿæˆé—®é¢˜åµŒå…¥
        query_embedding = EMBED_MODEL.encode([question]).tolist()
        
        progress(0.6, desc="æ£€ç´¢ç›¸å…³å†…å®¹...")
        # Chromaæ£€ç´¢
        results = COLLECTION.query(
            query_embeddings=query_embedding,
            n_results=5,
            include=['documents', 'metadatas']
        )
        
        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        sources_for_conflict_detection = []
        
        for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):
            source_type = metadata.get('source', 'æœ¬åœ°æ–‡æ¡£')
            
            source_item = {
                'text': doc,
                'type': source_type
            }
            
            if source_type == 'web':
                url = metadata.get('url', 'æœªçŸ¥URL')
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                context_with_sources.append(f"[ç½‘ç»œæ¥æº: {title}] (URL: {url})\n{doc}")
                source_item['url'] = url
                source_item['title'] = title
            else:
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                context_with_sources.append(f"[æœ¬åœ°æ–‡æ¡£: {source}]\n{doc}")
                source_item['source'] = source
            
            sources_for_conflict_detection.append(source_item)
        
        # æ£€æµ‹çŸ›ç›¾
        conflict_detected = detect_conflicts(sources_for_conflict_detection)
        
        # è·å–å¯ä¿¡æº
        if conflict_detected:
            credible_sources = [s for s in sources_for_conflict_detection 
                              if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]
        
        context = "\n\n".join(context_with_sources)
        
        # æ·»åŠ æ—¶é—´æ•æ„Ÿæ£€æµ‹
        time_sensitive = any(word in question for word in ["æœ€æ–°", "ä»Šå¹´", "å½“å‰", "æœ€è¿‘", "åˆšåˆš"])
        
        prompt_template = """åŸºäºä»¥ä¸‹{context_type}ï¼š
        {context}
        
        é—®é¢˜ï¼š{question}
        è¯·ç”¨ä¸­æ–‡ç»™å‡ºè¯¦ç»†å›ç­”ï¼Œå¹¶åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æºã€‚{time_note}{conflict_note}"""
        
        prompt = prompt_template.format(
            context_type="æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œæœç´¢ç»“æœ" if enable_web_search else "æœ¬åœ°æ–‡æ¡£",
            context=context,
            question=question,
            time_note="æ³¨æ„è¿™æ˜¯æ—¶é—´æ•æ„Ÿçš„é—®é¢˜ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨æœ€æ–°ä¿¡æ¯ã€‚" if time_sensitive and enable_web_search else "",
            conflict_note="\næ³¨æ„ï¼šæ£€æµ‹åˆ°ä¿¡æ¯æºä¹‹é—´å¯èƒ½å­˜åœ¨çŸ›ç›¾ï¼Œè¯·åœ¨å›ç­”ä¸­æ˜ç¡®æŒ‡å‡ºä¸åŒæ¥æºçš„å·®å¼‚ã€‚" if conflict_detected else ""
        )
        
        progress(0.8, desc="ç”Ÿæˆå›ç­”...")
        # è°ƒç”¨Ollama
        response = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "deepseek-r1:7b",
                "prompt": prompt,
                "stream": False
            },
            timeout=120,  # å»¶é•¿åˆ°2åˆ†é’Ÿ
            headers={'Connection': 'close'}  # æ·»åŠ è¿æ¥å¤´
        )
        response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 
        
        progress(1.0, desc="å®Œæˆ!")
        # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²å¹¶å¤„ç†ç©ºå€¼
        result = response.json()
        return str(result.get("response", "æœªè·å–åˆ°æœ‰æ•ˆå›ç­”"))
    except json.JSONDecodeError:
        return "å“åº”è§£æå¤±è´¥ï¼Œè¯·é‡è¯•"
    except KeyError:
        return "å“åº”æ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡"
    except Exception as e:
        progress(1.0, desc="é‡åˆ°é”™è¯¯")  # ç¡®ä¿è¿›åº¦æ¡å®Œæˆ
        return f"ç³»ç»Ÿé”™è¯¯: {str(e)}"

# ä¿®æ”¹ç•Œé¢å¸ƒå±€éƒ¨åˆ†
with gr.Blocks(
    title="æœ¬åœ°RAGé—®ç­”ç³»ç»Ÿ",
    css="""
    /* å…¨å±€ä¸»é¢˜å˜é‡ */
    :root[data-theme="light"] {
        --text-color: #2c3e50;
        --bg-color: #ffffff;
        --panel-bg: #f8f9fa;
        --border-color: #e9ecef;
        --success-color: #4CAF50;
        --error-color: #f44336;
        --primary-color: #2196F3;
        --secondary-bg: #ffffff;
        --hover-color: #e9ecef;
        --chat-user-bg: #e3f2fd;
        --chat-assistant-bg: #f5f5f5;
    }

    :root[data-theme="dark"] {
        --text-color: #e0e0e0;
        --bg-color: #1a1a1a;
        --panel-bg: #2d2d2d;
        --border-color: #404040;
        --success-color: #81c784;
        --error-color: #e57373;
        --primary-color: #64b5f6;
        --secondary-bg: #2d2d2d;
        --hover-color: #404040;
        --chat-user-bg: #1e3a5f;
        --chat-assistant-bg: #2d2d2d;
    }

    /* å…¨å±€æ ·å¼ */
    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }

    .gradio-container {
        max-width: 1200px !important;
        color: var(--text-color);
        background-color: var(--bg-color);
    }

    /* ä¸»é¢˜åˆ‡æ¢æŒ‰é’® */
    .theme-toggle {
        position: fixed;
        top: 20px;
        right: 20px;
        z-index: 1000;
        padding: 8px 16px;
        border-radius: 20px;
        border: 1px solid var(--border-color);
        background: var(--panel-bg);
        color: var(--text-color);
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 14px;
        display: flex;
        align-items: center;
        gap: 8px;
    }

    .theme-toggle:hover {
        background: var(--hover-color);
    }

    /* é¢æ¿æ ·å¼ */
    .left-panel {
        padding-right: 20px;
        border-right: 1px solid var(--border-color);
        background: var(--bg-color);
    }

    .right-panel {
        height: 100vh;
        background: var(--bg-color);
    }

    /* æ–‡ä»¶åˆ—è¡¨æ ·å¼ */
    .file-list {
        margin-top: 10px;
        padding: 12px;
        background: var(--panel-bg);
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.6;
        border: 1px solid var(--border-color);
    }

    /* ç­”æ¡ˆæ¡†æ ·å¼ */
    .answer-box {
        min-height: 500px !important;
        background: var(--panel-bg);
        border-radius: 8px;
        padding: 16px;
        font-size: 15px;
        line-height: 1.6;
        border: 1px solid var(--border-color);
    }

    /* è¾“å…¥æ¡†æ ·å¼ */
    textarea {
        background: var(--panel-bg) !important;
        color: var(--text-color) !important;
        border: 1px solid var(--border-color) !important;
        border-radius: 8px !important;
        padding: 12px !important;
        font-size: 14px !important;
    }

    /* æŒ‰é’®æ ·å¼ */
    button.primary {
        background: var(--primary-color) !important;
        color: white !important;
        border-radius: 8px !important;
        padding: 8px 16px !important;
        font-weight: 500 !important;
        transition: all 0.3s ease !important;
    }

    button.primary:hover {
        opacity: 0.9;
        transform: translateY(-1px);
    }

    /* æ ‡é¢˜å’Œæ–‡æœ¬æ ·å¼ */
    h1, h2, h3 {
        color: var(--text-color) !important;
        font-weight: 600 !important;
    }

    .footer-note {
        color: var(--text-color);
        opacity: 0.8;
        font-size: 13px;
        margin-top: 12px;
    }

    /* åŠ è½½å’Œè¿›åº¦æ ·å¼ */
    #loading, .progress-text {
        color: var(--text-color);
    }

    /* èŠå¤©è®°å½•æ ·å¼ */
    .chat-container {
        border: 1px solid var(--border-color);
        border-radius: 8px;
        margin-bottom: 16px;
        max-height: 600px;
        overflow-y: auto;
        background: var(--bg-color);
    }

    .chat-message {
        padding: 12px 16px;
        margin: 8px;
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.5;
    }

    .chat-message.user {
        background: var(--chat-user-bg);
        margin-left: 32px;
        border-top-right-radius: 4px;
    }

    .chat-message.assistant {
        background: var(--chat-assistant-bg);
        margin-right: 32px;
        border-top-left-radius: 4px;
    }

    .chat-message .timestamp {
        font-size: 12px;
        color: var(--text-color);
        opacity: 0.7;
        margin-bottom: 4px;
    }

    .chat-message .content {
        white-space: pre-wrap;
    }

    /* æŒ‰é’®ç»„æ ·å¼ */
    .button-row {
        display: flex;
        gap: 8px;
        margin-top: 8px;
    }

    .clear-button {
        background: var(--error-color) !important;
    }

    /* APIé…ç½®æç¤ºæ ·å¼ */
    .api-info {
        margin-top: 10px;
        padding: 10px;
        border-radius: 5px;
        background: var(--panel-bg);
        border: 1px solid var(--border-color);
    }
    """
) as demo:
    gr.Markdown("# ğŸ§  æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    
    with gr.Row():
        # å·¦ä¾§æ“ä½œé¢æ¿
        with gr.Column(scale=1, elem_classes="left-panel"):
            gr.Markdown("## ğŸ“‚ æ–‡æ¡£å¤„ç†åŒº")
            with gr.Group():
                file_input = gr.File(
                    label="ä¸Šä¼ PDFæ–‡æ¡£",
                    file_types=[".pdf"],
                    file_count="multiple"
                )
                upload_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                upload_status = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    interactive=False,
                    lines=2
                )
                file_list = gr.Textbox(
                    label="å·²å¤„ç†æ–‡ä»¶",
                    interactive=False,
                    lines=3,
                    elem_classes="file-list"
                )

        # å³ä¾§å¯¹è¯åŒº
        with gr.Column(scale=3, elem_classes="right-panel"):
            gr.Markdown("## ğŸ“ å¯¹è¯è®°å½•")
            
            # å¯¹è¯è®°å½•æ˜¾ç¤ºåŒº
            chatbot = gr.Chatbot(
                label="å¯¹è¯å†å²",
                height=500,
                elem_classes="chat-container",
                show_label=False
            )
            
            # é—®é¢˜è¾“å…¥åŒº
            with gr.Group():
                question_input = gr.Textbox(
                    label="è¾“å…¥é—®é¢˜",
                    lines=3,
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                    elem_id="question-input"
                )
                with gr.Row():
                    # æ·»åŠ è”ç½‘å¼€å…³
                    web_search_checkbox = gr.Checkbox(
                        label="å¯ç”¨è”ç½‘æœç´¢", 
                        value=False,
                        info="æ‰“å¼€åå°†åŒæ—¶æœç´¢ç½‘ç»œå†…å®¹ï¼ˆéœ€é…ç½®SERPAPI_KEYï¼‰"
                    )
                    
                with gr.Row():
                    ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary", scale=2)
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary", elem_classes="clear-button", scale=1)
                status_display = gr.HTML("", elem_id="status-display")
            
            # æ·»åŠ APIé…ç½®æç¤ºä¿¡æ¯
            api_info = gr.HTML(
                """
                <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
                    <p>ğŸ“¢ <strong>è”ç½‘åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
                    <p>1. éœ€è¦åœ¨é¡¹ç›®ç›®å½•ä¸‹çš„<code>.env</code>æ–‡ä»¶ä¸­é…ç½®<code>SERPAPI_KEY=æ‚¨çš„å¯†é’¥</code></p>
                    <p>2. å¯ä»¥åœ¨<a href="https://serpapi.com/" target="_blank">SerpAPIå®˜ç½‘</a>è·å–å…è´¹å¯†é’¥</p>
                </div>
                """
            )
            
            gr.Markdown("""
            <div class="footer-note">
                *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
            </div>
            """)

    # è°ƒæ•´åçš„åŠ è½½æç¤º
    gr.HTML("""
    <div id="loading" style="text-align:center;padding:20px;">
        <h3>ğŸ”„ ç³»ç»Ÿåˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™...</h3>
    </div>
    """)

    # è¿›åº¦æ˜¾ç¤ºç»„ä»¶è°ƒæ•´åˆ°å·¦ä¾§é¢æ¿ä¸‹æ–¹
    with gr.Row(visible=False) as progress_row:
        gr.HTML("""
        <div class="progress-text">
            <span>å½“å‰è¿›åº¦ï¼š</span>
            <span id="current-step" style="color: #2b6de3;">åˆå§‹åŒ–...</span>
            <span id="progress-percent" style="margin-left:15px;color: #e32b2b;">0%</span>
        </div>
        """)

    def clear_chat_history():
        return [], ""  # æ¸…ç©ºå¯¹è¯å†å²å’Œè¾“å…¥æ¡†

    # ä¿®æ”¹é—®ç­”å¤„ç†å‡½æ•°
    def process_chat(question, history, enable_web_search):
        if not question:
            return history, ""
        
        history = history or []
        history.append([question, None])
        
        try:
            for response, status in stream_answer(question, enable_web_search):
                if status != "é‡åˆ°é”™è¯¯":
                    history[-1][1] = response
                    yield history, ""
                else:
                    history[-1][1] = f"âŒ {response}"
                    yield history, ""
        except Exception as e:
            history[-1][1] = f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}"
            yield history, ""

    # æ£€æŸ¥SERPAPIé…ç½®çŠ¶æ€å¹¶æ›´æ–°æç¤ºä¿¡æ¯
    def update_api_info(enable_web_search):
        if not enable_web_search:
            return """
            <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
                <p>ğŸ“¢ <strong>è”ç½‘åŠŸèƒ½å·²å…³é—­</strong></p>
                <p>å¼€å¯è”ç½‘åŠŸèƒ½å¯è·å–æœ€æ–°ç½‘ç»œä¿¡æ¯</p>
            </div>
            """
        
        if check_serpapi_key():
            return """
            <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);border-left:4px solid #4CAF50;">
                <p>âœ… <strong>è”ç½‘åŠŸèƒ½å·²å¯ç”¨</strong></p>
                <p>SERPAPI_KEYå·²é…ç½®ï¼Œå¯ä»¥è¿›è¡Œç½‘ç»œæœç´¢</p>
            </div>
            """
        else:
            return """
            <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);border-left:4px solid #f44336;">
                <p>âŒ <strong>è”ç½‘åŠŸèƒ½å¯ç”¨å¤±è´¥</strong></p>
                <p>æœªæ£€æµ‹åˆ°SERPAPI_KEYé…ç½®ï¼Œè¯·åœ¨é¡¹ç›®ç›®å½•ä¸‹çš„<code>.env</code>æ–‡ä»¶ä¸­æ·»åŠ ï¼š</p>
                <pre style="background:var(--code-bg);padding:5px;border-radius:3px;">SERPAPI_KEY=æ‚¨çš„APIå¯†é’¥</pre>
                <p>å¯ä»¥åœ¨<a href="https://serpapi.com/" target="_blank">SerpAPIå®˜ç½‘</a>è·å–å…è´¹å¯†é’¥</p>
            </div>
            """

    # æ›´æ–°äº‹ä»¶å¤„ç†
    web_search_checkbox.change(
        fn=update_api_info,
        inputs=web_search_checkbox, 
        outputs=api_info
    )
    
    ask_btn.click(
        fn=process_chat,
        inputs=[question_input, chatbot, web_search_checkbox],
        outputs=[chatbot, question_input],
        show_progress=False
    ).then(
        fn=lambda: "",
        outputs=status_display
    )

    clear_btn.click(
        fn=clear_chat_history,
        outputs=[chatbot, question_input],
        show_progress=False
    )

    # æ·»åŠ æ–‡ä»¶å¤„ç†æŒ‰é’®äº‹ä»¶
    upload_btn.click(
        fn=process_multiple_pdfs,
        inputs=file_input,
        outputs=[upload_status, file_list]
    )

# ä¿®æ”¹JavaScriptæ³¨å…¥éƒ¨åˆ†
demo._js = """
function gradioApp() {
    // è®¾ç½®é»˜è®¤ä¸»é¢˜ä¸ºæš—è‰²
    document.documentElement.setAttribute('data-theme', 'dark');
    
    const observer = new MutationObserver((mutations) => {
        document.getElementById("loading").style.display = "none";
        const progress = document.querySelector('.progress-text');
        if (progress) {
            const percent = document.querySelector('.progress > div')?.innerText || '';
            const step = document.querySelector('.progress-description')?.innerText || '';
            document.getElementById('current-step').innerText = step;
            document.getElementById('progress-percent').innerText = percent;
        }
    });
    observer.observe(document.body, {childList: true, subtree: true});
}

function toggleTheme() {
    const root = document.documentElement;
    const currentTheme = root.getAttribute('data-theme');
    const newTheme = currentTheme === 'light' ? 'dark' : 'light';
    root.setAttribute('data-theme', newTheme);
}

// åˆå§‹åŒ–ä¸»é¢˜
document.addEventListener('DOMContentLoaded', () => {
    document.documentElement.setAttribute('data-theme', 'dark');
});
"""

# ä¿®æ”¹ç«¯å£æ£€æŸ¥å‡½æ•°
def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0  # æ›´å¯é çš„æ£€æµ‹æ–¹å¼

def check_environment():
    """ç¯å¢ƒä¾èµ–æ£€æŸ¥"""
    try:
        # æ·»åŠ æ¨¡å‹å­˜åœ¨æ€§æ£€æŸ¥
        model_check = session.post(
            "http://localhost:11434/api/show",
            json={"name": "deepseek-r1:7b"},
            timeout=10
        )
        if model_check.status_code != 200:
            print("æ¨¡å‹æœªåŠ è½½ï¼è¯·å…ˆæ‰§è¡Œï¼š")
            print("ollama pull deepseek-r1:7b")
            return False
            
        # åŸæœ‰æ£€æŸ¥ä¿æŒä¸å˜...
        response = session.get(
            "http://localhost:11434/api/tags",
            proxies={"http": None, "https": None},  # ç¦ç”¨ä»£ç†
            timeout=5
        )
        if response.status_code != 200:
            print("OllamaæœåŠ¡å¼‚å¸¸ï¼Œè¿”å›çŠ¶æ€ç :", response.status_code)
            return False
        return True
    except Exception as e:
        print("Ollamaè¿æ¥å¤±è´¥:", str(e))
        return False

# æ–¹æ¡ˆ2ï¼šç¦ç”¨æµè§ˆå™¨ç¼“å­˜ï¼ˆæ·»åŠ metaæ ‡ç­¾ï¼‰
gr.HTML("""
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Expires" content="0">
""")

# æ¢å¤ä¸»ç¨‹åºå¯åŠ¨éƒ¨åˆ†
if __name__ == "__main__":
    if not check_environment():
        exit(1)
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)
    
    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)
        
    try:
        ollama_check = session.get("http://localhost:11434", timeout=5)
        if ollama_check.status_code != 200:
            print("OllamaæœåŠ¡æœªæ­£å¸¸å¯åŠ¨ï¼")
            print("è¯·å…ˆæ‰§è¡Œï¼šollama serve å¯åŠ¨æœåŠ¡")
            exit(1)
            
        webbrowser.open(f"http://127.0.0.1:{selected_port}")
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}")

